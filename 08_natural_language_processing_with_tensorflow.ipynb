{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP fundamentals in TensorFlow\n",
    "\n",
    "NLP has the goal of deriving information out of natural language (could be sequences of text or speech).\n",
    "\n",
    "Another common term for NLP problem is sequence to sequence problems (seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 12 13:05:44 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.59       Driver Version: 516.59       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   56C    P8    12W /  N/A |    241MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1960    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5808    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     10140    C+G   ...zilla Firefox\\firefox.exe    N/A      |\n",
      "|    0   N/A  N/A     15156    C+G   ...signal-desktop\\Signal.exe    N/A      |\n",
      "|    0   N/A  N/A     19628    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     20088    C+G   ... iCUE 4 Software\\iCUE.exe    N/A      |\n",
      "|    0   N/A  N/A     21248    C+G   ...ropbox\\Client\\Dropbox.exe    N/A      |\n",
      "|    0   N/A  N/A     21312    C+G   ...264.49\\msedgewebview2.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n",
      "GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU (UUID: GPU-2349283b-d8c3-574d-bb05-0097332584c3)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"helper_functions.py\"):\n",
    "    !python -m wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
    "\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a text dataset\n",
    "\n",
    "The dataset we're going to be using is Kaggle's introduction to NLP dataset (text samples of Tweets labelled as disaster or not disaster).\n",
    "\n",
    "See the original source: https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"nlp_getting_started.zip\"):\n",
    "    !python -m wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
    "unzip_data(\"C:/Selbststudium/Udemy/Udemy_TensorFlow_Certificate/nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "To visualize our text samples, we first have to read them in , one way to do so would be to use Python: https://realpython.com/read-write-files-python/ \n",
    "\n",
    "But i prefer to get visual straight away.\n",
    "\n",
    "So another way to do this is to use pandes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Forest fire near La Ronge Sask. Canada', 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"text\"][1], train_df[\"target\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ¬â√õ√èThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ¬â√õ√èThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the test dataframe look like?\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total samples?\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text: Swansea ¬â√õ√∑plot hijack transfer move for Southampton target Virgil van Dijk¬â√õ¬™ http://t.co/PVmr38LnvA\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text: 'If you looking for my niggas you can follow the sirens.' ????\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text: @Benji_Devos thanks thanks :3\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text: The Twitter update pretty much wrecked the app\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text: Looks and sounds like a war zone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5) # Create random indexes not higher than the total number\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split training data into training and validation sets \n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    train_df_shuffled[\"text\"].to_numpy(),\n",
    "    train_df_shuffled[\"target\"].to_numpy(),\n",
    "    test_size=0.1,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 762, 6851, 762)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(val_sentences), len(train_labels), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 10 samples\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cnverting text into numbers\n",
    "\n",
    "When dealing with a text problem, one of the first thing you'll have to do before you can build a model is to convert your text to numbers. \n",
    "\n",
    "There are a few ways to do this, namely:\n",
    "* Tokenization - direct mapping of token (a token could be a word or a character)\n",
    "* Embedding - create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "       'Imagine getting flattened by Kurt Zouma',\n",
       "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['@mogacola', '@zamtriossu', 'i', 'screamed', 'after', 'hitting', 'tweet'], 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0].split(), len(train_sentences[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the average number of tokes (words) in the training tweets\n",
    "max_length = round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# Use the default TextVectorization parameters\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    standardize=\"lower_and_strip_punctuation\", # Default\n",
    "    split=\"whitespace\", # Default\n",
    "    ngrams=None, # Default\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    pad_to_max_tokens=True) # Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Megadeth Week - Symphony Of Destruction http://t.co/ECd7HiZja1\n",
      "    Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[   1,  561, 4479,    6,  399,    1,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\n\\\n",
    "    Vectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " ['', '[UNK]', 'the', 'a', 'in'],\n",
       " ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "len(words_in_vocab), top_5_words, bottom_5_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Embedding using an EmbeddingLayer\n",
    "\n",
    "To make our embedding, we're going to use TensorFlow's EmbeddingLayer: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "The parameters we care most about for our embedding layer: \n",
    "* `input_dim` = the size of our vocabulary\n",
    "* `output_dim` = the size of the output embedding vector, for example, a value of 100 would mean each tolen gets represented by a vector 100 long\n",
    "* `input_length` = length of the sequences being passed to the embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x16db2196100>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(\n",
    "    input_dim=max_vocab_length,\n",
    "    output_dim=128,\n",
    "    input_length=max_length)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "#bioterrorism Authorities allay #glanders fears ahead of Rio Olympic equestrian test event http://t.co/UotPNSQpz5 via @HorsetalkNZ    \n",
      "Embedded version: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.03486104,  0.0029893 , -0.00246152, ..., -0.03333913,\n",
       "         -0.01164166,  0.0320271 ],\n",
       "        [-0.04521016, -0.01056033,  0.03897185, ..., -0.03736807,\n",
       "         -0.03224889,  0.0451399 ],\n",
       "        [-0.0164845 , -0.0186978 , -0.01641975, ...,  0.02697707,\n",
       "         -0.02946275,  0.04060824],\n",
       "        ...,\n",
       "        [-0.0164845 , -0.0186978 , -0.01641975, ...,  0.02697707,\n",
       "         -0.02946275,  0.04060824],\n",
       "        [-0.01463867,  0.03272406,  0.0138428 , ...,  0.00716169,\n",
       "          0.04657973, -0.03294163],\n",
       "        [-0.0164845 , -0.0186978 , -0.01641975, ...,  0.02697707,\n",
       "         -0.02946275,  0.04060824]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "    \\nEmbedded version: \")\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([ 0.03486104,  0.0029893 , -0.00246152, -0.03160349, -0.00057074,\n",
       "        -0.04700584,  0.00889476, -0.04995942, -0.00167131,  0.03905557,\n",
       "         0.0289961 ,  0.04233566,  0.02514123,  0.02886802, -0.00734575,\n",
       "         0.044863  , -0.01356035,  0.01591469,  0.00839349, -0.03647577,\n",
       "        -0.04164805, -0.02916286, -0.01558067, -0.02042607, -0.00758855,\n",
       "        -0.02593992,  0.01978464, -0.03038995,  0.0063877 ,  0.02705929,\n",
       "        -0.02001973,  0.01687608,  0.00157117,  0.02969011, -0.02433693,\n",
       "         0.02115684, -0.01299958, -0.0316118 , -0.03431211,  0.04609041,\n",
       "         0.0412867 ,  0.03265006,  0.04807356,  0.02636014, -0.04342666,\n",
       "         0.04591818, -0.00540435, -0.00288951, -0.03174573,  0.00662764,\n",
       "        -0.03077923,  0.00990288,  0.0028479 ,  0.01766348, -0.00436   ,\n",
       "         0.01499588, -0.01638198,  0.04986138, -0.03455976, -0.00688618,\n",
       "        -0.02060212,  0.00450531,  0.01151477,  0.0326442 ,  0.0493513 ,\n",
       "         0.00825642, -0.02020172,  0.02025745, -0.02917208, -0.00880303,\n",
       "         0.00089791, -0.02533076, -0.04805887,  0.02797185, -0.03465604,\n",
       "        -0.01696179, -0.04860004,  0.03223756,  0.01689463, -0.01912997,\n",
       "        -0.0182395 ,  0.0359859 ,  0.04284226, -0.01431055, -0.00856073,\n",
       "         0.00227772, -0.0372734 ,  0.0289897 , -0.01740944,  0.04467192,\n",
       "        -0.02788916,  0.00948653,  0.03596917,  0.04111845,  0.01901618,\n",
       "         0.0321759 , -0.0340732 , -0.02607008, -0.03305676, -0.03317903,\n",
       "        -0.01318137, -0.00736481,  0.0499602 ,  0.02751262,  0.01665839,\n",
       "         0.02555207,  0.00664859, -0.02900312, -0.04749736,  0.00942582,\n",
       "         0.01899376, -0.02241056, -0.04310444, -0.00773753,  0.00344664,\n",
       "        -0.01885404, -0.02821555, -0.00589808,  0.0418648 ,  0.03724312,\n",
       "        -0.04850531,  0.03978217,  0.0244346 ,  0.04051511, -0.02245895,\n",
       "        -0.03333913, -0.01164166,  0.0320271 ], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " '#bioterrorism Authorities allay #glanders fears ahead of Rio Olympic equestrian test event http://t.co/UotPNSQpz5 via @HorsetalkNZ')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling a text dataset (running a series of experiments)\n",
    "\n",
    "Now we've got a way to turn our text sequences into numbers, it's time  to start building a series of modelling experiments.\n",
    "\n",
    "We'll start with a baseline and move on from there.\n",
    "\n",
    "* Model 0: Naive Bayes (baseline), from sklearn ML map: \n",
    "* Model 1: Feed-forward neural network (dense model)\n",
    "* Model 2: LSTM (RNN)\n",
    "* Model 3: GRU (RNN) \n",
    "* Model 4: Bidirectional-LSTM model (RNN)\n",
    "* Model 5: 1D Convolutional Network (CNN)\n",
    "* Model 6: TensorFlow Hub Pretrained Feature Extractor (using transfer learning for NLP)\n",
    "* Model 7: Same as model 6 with 10% of training data\n",
    "\n",
    "How are we going to approach all of these?\n",
    "\n",
    "Use the standard steps in modelling with tensorflow:\n",
    "* Create a model\n",
    "* Build the model\n",
    "* Fit the model\n",
    "* Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "As with all machine learning modelling experiments it's important to create a baseline model so you've got a benchmark for future experiments to build uopn.\n",
    "\n",
    "To create our baseline, we'll use Sklearn's Mulinomial Naive Bayes using the TF-IDF formula to convert our words to numbers.\n",
    "\n",
    "> üîë **Note:** It's common to use non DL-algorithms as a baseline because of their speed and then later using DL to see if you can improve upon them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([ # Pipeline is similar to tensorflow's sequential\n",
    "    (\"tfidf\", TfidfVectorizer()), # name of the step is \"tfidf\"; convert words to numbers using tfidf\n",
    "    (\"clf\", MultinomialNB()), # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an evaluation function for our model experiments\n",
    "\n",
    "We could evaluate all of our model's predictions with different metrics every time, however this will be cumbersome and could easily be fixed with a function.\n",
    "\n",
    "Let's create one to compare our model's predictions with the truth labels using the following metrics:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "\n",
    "For a deep overview of many different methods, see the Sklearn documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate accuracy, precision, reall and f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates model accuracy, precision, recall and f1 score of a binare classification model.\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    # Calculate model precision, recall and f1-score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _, = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\n",
    "        \"accuracy\": model_accuracy, \n",
    "        \"precision\": model_precision,\n",
    "        \"recall\": model_recall, \n",
    "        \"f1-score\": model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1-score': 0.7862189758049549}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: A simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard callback (Need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save tensorboard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the feature vector for each token to one vector\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20220712-130550\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 7ms/step - loss: 0.6134 - accuracy: 0.6929 - val_loss: 0.5377 - val_accuracy: 0.7572\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4425 - accuracy: 0.8175 - val_loss: 0.4694 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.3478 - accuracy: 0.8613 - val_loss: 0.4572 - val_accuracy: 0.7887\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.8897 - val_loss: 0.4649 - val_accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.2385 - accuracy: 0.9120 - val_loss: 0.4762 - val_accuracy: 0.7808\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(\n",
    "    x=train_sentences,\n",
    "    y=train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        dir_name=SAVE_DIR,\n",
    "        experiment_name=\"model_1_dense\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'accuracy': 79.26509186351706,\n",
       "  'precision': 0.8111390004213173,\n",
       "  'recall': 0.7926509186351706,\n",
       "  'f1-score': 0.7862189758049549},)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.4762 - accuracy: 0.7808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47624942660331726, 0.7808399200439453]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43768293],\n",
       "       [0.75415933],\n",
       "       [0.9975879 ],\n",
       "       [0.14311554],\n",
       "       [0.13520585],\n",
       "       [0.9465931 ],\n",
       "       [0.9103034 ],\n",
       "       [0.9930292 ],\n",
       "       [0.9723433 ],\n",
       "       [0.313366  ],\n",
       "       [0.12725355],\n",
       "       [0.7057136 ],\n",
       "       [0.0632465 ],\n",
       "       [0.20980069],\n",
       "       [0.00520669],\n",
       "       [0.15502185],\n",
       "       [0.03070253],\n",
       "       [0.09462406],\n",
       "       [0.29177612],\n",
       "       [0.6057108 ],\n",
       "       [0.9190093 ],\n",
       "       [0.0468171 ],\n",
       "       [0.4378229 ],\n",
       "       [0.10253182],\n",
       "       [0.95873827],\n",
       "       [0.9991129 ],\n",
       "       [0.04005963],\n",
       "       [0.08359136],\n",
       "       [0.03313359],\n",
       "       [0.23088615],\n",
       "       [0.631817  ],\n",
       "       [0.2762977 ],\n",
       "       [0.5204283 ],\n",
       "       [0.18090728],\n",
       "       [0.5496958 ],\n",
       "       [0.06621452],\n",
       "       [0.9951564 ],\n",
       "       [0.17166887],\n",
       "       [0.03947314],\n",
       "       [0.99830985],\n",
       "       [0.20945258],\n",
       "       [0.02699536],\n",
       "       [0.3212691 ],\n",
       "       [0.06764256],\n",
       "       [0.64536524],\n",
       "       [0.9827754 ],\n",
       "       [0.3214004 ],\n",
       "       [0.9379389 ],\n",
       "       [0.22752221],\n",
       "       [0.6685029 ],\n",
       "       [0.06853318],\n",
       "       [0.5533916 ],\n",
       "       [0.45756432],\n",
       "       [0.03354051],\n",
       "       [0.11359986],\n",
       "       [0.04034684],\n",
       "       [0.24280183],\n",
       "       [0.95966583],\n",
       "       [0.13610364],\n",
       "       [0.0030834 ],\n",
       "       [0.15663376],\n",
       "       [0.9703169 ],\n",
       "       [0.93499047],\n",
       "       [0.19286321],\n",
       "       [0.9357563 ],\n",
       "       [0.9829846 ],\n",
       "       [0.75842226],\n",
       "       [0.4543835 ],\n",
       "       [0.10918297],\n",
       "       [0.16469067],\n",
       "       [0.08795319],\n",
       "       [0.04555765],\n",
       "       [0.94266427],\n",
       "       [0.15268748],\n",
       "       [0.17352743],\n",
       "       [0.4651862 ],\n",
       "       [0.4346152 ],\n",
       "       [0.82493997],\n",
       "       [0.28319275],\n",
       "       [0.5556076 ],\n",
       "       [0.5163155 ],\n",
       "       [0.29546902],\n",
       "       [0.996766  ],\n",
       "       [0.1145948 ],\n",
       "       [0.18730623],\n",
       "       [0.12104481],\n",
       "       [0.02409907],\n",
       "       [0.10138765],\n",
       "       [0.6757047 ],\n",
       "       [0.9130555 ],\n",
       "       [0.9920161 ],\n",
       "       [0.01474752],\n",
       "       [0.57123494],\n",
       "       [0.03537256],\n",
       "       [0.9844236 ],\n",
       "       [0.7481896 ],\n",
       "       [0.8624231 ],\n",
       "       [0.9767957 ],\n",
       "       [0.8992736 ],\n",
       "       [0.964869  ],\n",
       "       [0.9992968 ],\n",
       "       [0.18151362],\n",
       "       [0.01527142],\n",
       "       [0.91627246],\n",
       "       [0.8906198 ],\n",
       "       [0.09682886],\n",
       "       [0.8900533 ],\n",
       "       [0.9788831 ],\n",
       "       [0.08129539],\n",
       "       [0.531329  ],\n",
       "       [0.7668663 ],\n",
       "       [0.04288361],\n",
       "       [0.23842841],\n",
       "       [0.17107557],\n",
       "       [0.18504843],\n",
       "       [0.5116598 ],\n",
       "       [0.5154267 ],\n",
       "       [0.7709001 ],\n",
       "       [0.79948336],\n",
       "       [0.10904632],\n",
       "       [0.99978   ],\n",
       "       [0.10907815],\n",
       "       [0.1430522 ],\n",
       "       [0.85260165],\n",
       "       [0.47811466],\n",
       "       [0.3340415 ],\n",
       "       [0.8364815 ],\n",
       "       [0.01030353],\n",
       "       [0.0751522 ],\n",
       "       [0.8120036 ],\n",
       "       [0.10819928],\n",
       "       [0.99978   ],\n",
       "       [0.9998275 ],\n",
       "       [0.9991129 ],\n",
       "       [0.984683  ],\n",
       "       [0.08689547],\n",
       "       [0.97786844],\n",
       "       [0.21882027],\n",
       "       [0.32697383],\n",
       "       [0.09060191],\n",
       "       [0.99656916],\n",
       "       [0.29826748],\n",
       "       [0.20980069],\n",
       "       [0.9530036 ],\n",
       "       [0.27683225],\n",
       "       [0.64780265],\n",
       "       [0.04916351],\n",
       "       [0.00973589],\n",
       "       [0.2775258 ],\n",
       "       [0.9814177 ],\n",
       "       [0.34154552],\n",
       "       [0.09463588],\n",
       "       [0.5004985 ],\n",
       "       [0.21390902],\n",
       "       [0.24677771],\n",
       "       [0.99197817],\n",
       "       [0.84430844],\n",
       "       [0.5587829 ],\n",
       "       [0.99046016],\n",
       "       [0.02641649],\n",
       "       [0.9753636 ],\n",
       "       [0.06646631],\n",
       "       [0.26999655],\n",
       "       [0.9920536 ],\n",
       "       [0.2515122 ],\n",
       "       [0.08793543],\n",
       "       [0.9985304 ],\n",
       "       [0.35517377],\n",
       "       [0.98249143],\n",
       "       [0.23505563],\n",
       "       [0.9936167 ],\n",
       "       [0.88556176],\n",
       "       [0.81312454],\n",
       "       [0.03753904],\n",
       "       [0.9982437 ],\n",
       "       [0.07713445],\n",
       "       [0.4016021 ],\n",
       "       [0.50094646],\n",
       "       [0.7430941 ],\n",
       "       [0.9952122 ],\n",
       "       [0.02179912],\n",
       "       [0.87589735],\n",
       "       [0.8375745 ],\n",
       "       [0.9769438 ],\n",
       "       [0.98183745],\n",
       "       [0.43003005],\n",
       "       [0.10609033],\n",
       "       [0.999443  ],\n",
       "       [0.01570989],\n",
       "       [0.03097824],\n",
       "       [0.13107964],\n",
       "       [0.92726576],\n",
       "       [0.11575179],\n",
       "       [0.31282046],\n",
       "       [0.01572929],\n",
       "       [0.12016524],\n",
       "       [0.04626989],\n",
       "       [0.2509392 ],\n",
       "       [0.7950436 ],\n",
       "       [0.08480559],\n",
       "       [0.29237598],\n",
       "       [0.90213466],\n",
       "       [0.9763555 ],\n",
       "       [0.38132176],\n",
       "       [0.150226  ],\n",
       "       [0.9998634 ],\n",
       "       [0.56207883],\n",
       "       [0.9234307 ],\n",
       "       [0.6914609 ],\n",
       "       [0.8537324 ],\n",
       "       [0.34030473],\n",
       "       [0.98628277],\n",
       "       [0.02105046],\n",
       "       [0.24664275],\n",
       "       [0.00794063],\n",
       "       [0.00547692],\n",
       "       [0.9557483 ],\n",
       "       [0.839925  ],\n",
       "       [0.9143993 ],\n",
       "       [0.17548172],\n",
       "       [0.7878106 ],\n",
       "       [0.10523336],\n",
       "       [0.02735322],\n",
       "       [0.21268047],\n",
       "       [0.9838552 ],\n",
       "       [0.2362103 ],\n",
       "       [0.57509696],\n",
       "       [0.99473405],\n",
       "       [0.62276   ],\n",
       "       [0.65254253],\n",
       "       [0.12075678],\n",
       "       [0.27254182],\n",
       "       [0.82841295],\n",
       "       [0.28231815],\n",
       "       [0.55824023],\n",
       "       [0.18585269],\n",
       "       [0.58212286],\n",
       "       [0.38580066],\n",
       "       [0.20595391],\n",
       "       [0.0903504 ],\n",
       "       [0.42317927],\n",
       "       [0.2961768 ],\n",
       "       [0.99988675],\n",
       "       [0.9876123 ],\n",
       "       [0.09709986],\n",
       "       [0.02488581],\n",
       "       [0.8831644 ],\n",
       "       [0.1165259 ],\n",
       "       [0.10861264],\n",
       "       [0.5132796 ],\n",
       "       [0.05001894],\n",
       "       [0.6069771 ],\n",
       "       [0.00113669],\n",
       "       [0.4557506 ],\n",
       "       [0.93467176],\n",
       "       [0.2496386 ],\n",
       "       [0.9759266 ],\n",
       "       [0.99903107],\n",
       "       [0.306952  ],\n",
       "       [0.18505026],\n",
       "       [0.37082648],\n",
       "       [0.02630369],\n",
       "       [0.00491518],\n",
       "       [0.98446333],\n",
       "       [0.97298414],\n",
       "       [0.74603206],\n",
       "       [0.9627987 ],\n",
       "       [0.0592078 ],\n",
       "       [0.19415613],\n",
       "       [0.01031753],\n",
       "       [0.21380965],\n",
       "       [0.03667415],\n",
       "       [0.9628756 ],\n",
       "       [0.11759882],\n",
       "       [0.01125313],\n",
       "       [0.9743566 ],\n",
       "       [0.01500238],\n",
       "       [0.1160899 ],\n",
       "       [0.98174435],\n",
       "       [0.04477642],\n",
       "       [0.08919096],\n",
       "       [0.00867209],\n",
       "       [0.9722911 ],\n",
       "       [0.57970595],\n",
       "       [0.69312274],\n",
       "       [0.7228989 ],\n",
       "       [0.59766376],\n",
       "       [0.06551225],\n",
       "       [0.93765974],\n",
       "       [0.04059534],\n",
       "       [0.7626528 ],\n",
       "       [0.42744017],\n",
       "       [0.45899975],\n",
       "       [0.39961028],\n",
       "       [0.2307365 ],\n",
       "       [0.7158387 ],\n",
       "       [0.2311355 ],\n",
       "       [0.695205  ],\n",
       "       [0.14783995],\n",
       "       [0.8384947 ],\n",
       "       [0.05744486],\n",
       "       [0.09641741],\n",
       "       [0.2849485 ],\n",
       "       [0.9838187 ],\n",
       "       [0.19487028],\n",
       "       [0.10847499],\n",
       "       [0.34812817],\n",
       "       [0.2433826 ],\n",
       "       [0.11254963],\n",
       "       [0.03059731],\n",
       "       [0.03550744],\n",
       "       [0.9811943 ],\n",
       "       [0.39840618],\n",
       "       [0.30231252],\n",
       "       [0.99982375],\n",
       "       [0.063274  ],\n",
       "       [0.71626025],\n",
       "       [0.3024641 ],\n",
       "       [0.05084414],\n",
       "       [0.18138106],\n",
       "       [0.20428179],\n",
       "       [0.11818408],\n",
       "       [0.92718214],\n",
       "       [0.29637682],\n",
       "       [0.9865183 ],\n",
       "       [0.09384737],\n",
       "       [0.02586589],\n",
       "       [0.99560696],\n",
       "       [0.02392396],\n",
       "       [0.99667776],\n",
       "       [0.17914727],\n",
       "       [0.04434108],\n",
       "       [0.9691083 ],\n",
       "       [0.05442194],\n",
       "       [0.0423285 ],\n",
       "       [0.9805093 ],\n",
       "       [0.00631523],\n",
       "       [0.22579965],\n",
       "       [0.7835328 ],\n",
       "       [0.921962  ],\n",
       "       [0.00488542],\n",
       "       [0.1743606 ],\n",
       "       [0.9815818 ],\n",
       "       [0.96997166],\n",
       "       [0.77397573],\n",
       "       [0.45077097],\n",
       "       [0.6464621 ],\n",
       "       [0.4708845 ],\n",
       "       [0.06681967],\n",
       "       [0.11854447],\n",
       "       [0.11640313],\n",
       "       [0.7041215 ],\n",
       "       [0.0477439 ],\n",
       "       [0.4497349 ],\n",
       "       [0.41856122],\n",
       "       [0.01407202],\n",
       "       [0.9715612 ],\n",
       "       [0.9991129 ],\n",
       "       [0.99322754],\n",
       "       [0.03592679],\n",
       "       [0.33725348],\n",
       "       [0.15999782],\n",
       "       [0.39449856],\n",
       "       [0.8233163 ],\n",
       "       [0.1261831 ],\n",
       "       [0.02495562],\n",
       "       [0.07553918],\n",
       "       [0.04711018],\n",
       "       [0.59417284],\n",
       "       [0.0106661 ],\n",
       "       [0.2386344 ],\n",
       "       [0.03883009],\n",
       "       [0.44556758],\n",
       "       [0.43394768],\n",
       "       [0.34926644],\n",
       "       [0.21328105],\n",
       "       [0.06255861],\n",
       "       [0.36206856],\n",
       "       [0.08878922],\n",
       "       [0.9969591 ],\n",
       "       [0.9257324 ],\n",
       "       [0.4545849 ],\n",
       "       [0.6544394 ],\n",
       "       [0.02301021],\n",
       "       [0.5820051 ],\n",
       "       [0.9920312 ],\n",
       "       [0.6926658 ],\n",
       "       [0.21883698],\n",
       "       [0.9829506 ],\n",
       "       [0.22687474],\n",
       "       [0.95085305],\n",
       "       [0.31640786],\n",
       "       [0.02866524],\n",
       "       [0.6043291 ],\n",
       "       [0.45707247],\n",
       "       [0.99672997],\n",
       "       [0.12166905],\n",
       "       [0.06219935],\n",
       "       [0.08284789],\n",
       "       [0.1261831 ],\n",
       "       [0.99904007],\n",
       "       [0.03637243],\n",
       "       [0.6814144 ],\n",
       "       [0.9753813 ],\n",
       "       [0.08729956],\n",
       "       [0.99978   ],\n",
       "       [0.03925489],\n",
       "       [0.40441713],\n",
       "       [0.06228431],\n",
       "       [0.80663514],\n",
       "       [0.9302027 ],\n",
       "       [0.06770603],\n",
       "       [0.00221766],\n",
       "       [0.2504602 ],\n",
       "       [0.99091184],\n",
       "       [0.83716094],\n",
       "       [0.14126948],\n",
       "       [0.51342195],\n",
       "       [0.76148546],\n",
       "       [0.02834149],\n",
       "       [0.9931624 ],\n",
       "       [0.58155113],\n",
       "       [0.9989139 ],\n",
       "       [0.9357579 ],\n",
       "       [0.10930211],\n",
       "       [0.3768898 ],\n",
       "       [0.10978659],\n",
       "       [0.93055177],\n",
       "       [0.75653595],\n",
       "       [0.44267696],\n",
       "       [0.03655926],\n",
       "       [0.37233183],\n",
       "       [0.05010447],\n",
       "       [0.074653  ],\n",
       "       [0.15579061],\n",
       "       [0.39798385],\n",
       "       [0.51047915],\n",
       "       [0.18795224],\n",
       "       [0.9991942 ],\n",
       "       [0.98523825],\n",
       "       [0.29016367],\n",
       "       [0.75415933],\n",
       "       [0.23520017],\n",
       "       [0.04548565],\n",
       "       [0.3453409 ],\n",
       "       [0.7545611 ],\n",
       "       [0.10118715],\n",
       "       [0.25037476],\n",
       "       [0.01830887],\n",
       "       [0.53854966],\n",
       "       [0.00248478],\n",
       "       [0.9659855 ],\n",
       "       [0.9793809 ],\n",
       "       [0.99449843],\n",
       "       [0.9731425 ],\n",
       "       [0.8292561 ],\n",
       "       [0.20525151],\n",
       "       [0.0607458 ],\n",
       "       [0.69038576],\n",
       "       [0.9162074 ],\n",
       "       [0.9985405 ],\n",
       "       [0.00310932],\n",
       "       [0.15824653],\n",
       "       [0.19991772],\n",
       "       [0.9990502 ],\n",
       "       [0.99937207],\n",
       "       [0.26780677],\n",
       "       [0.14639004],\n",
       "       [0.9969156 ],\n",
       "       [0.05551391],\n",
       "       [0.36067215],\n",
       "       [0.9865339 ],\n",
       "       [0.15517174],\n",
       "       [0.04530151],\n",
       "       [0.9409692 ],\n",
       "       [0.32475796],\n",
       "       [0.46177265],\n",
       "       [0.9701046 ],\n",
       "       [0.02896019],\n",
       "       [0.06667927],\n",
       "       [0.01134473],\n",
       "       [0.01969743],\n",
       "       [0.12539919],\n",
       "       [0.976132  ],\n",
       "       [0.02108294],\n",
       "       [0.454142  ],\n",
       "       [0.53465855],\n",
       "       [0.07486442],\n",
       "       [0.281122  ],\n",
       "       [0.07621397],\n",
       "       [0.38965502],\n",
       "       [0.9973911 ],\n",
       "       [0.61073345],\n",
       "       [0.16059984],\n",
       "       [0.133875  ],\n",
       "       [0.14649639],\n",
       "       [0.06605953],\n",
       "       [0.7160081 ],\n",
       "       [0.02820062],\n",
       "       [0.88683754],\n",
       "       [0.819743  ],\n",
       "       [0.56184375],\n",
       "       [0.6991921 ],\n",
       "       [0.79414636],\n",
       "       [0.11456513],\n",
       "       [0.31570596],\n",
       "       [0.42844138],\n",
       "       [0.92719424],\n",
       "       [0.2161046 ],\n",
       "       [0.25974065],\n",
       "       [0.411143  ],\n",
       "       [0.01480575],\n",
       "       [0.13795376],\n",
       "       [0.39103514],\n",
       "       [0.81527126],\n",
       "       [0.13530102],\n",
       "       [0.99338865],\n",
       "       [0.915607  ],\n",
       "       [0.7481896 ],\n",
       "       [0.9843338 ],\n",
       "       [0.16150711],\n",
       "       [0.07690244],\n",
       "       [0.90876627],\n",
       "       [0.18833841],\n",
       "       [0.02200423],\n",
       "       [0.09709506],\n",
       "       [0.15822275],\n",
       "       [0.00133892],\n",
       "       [0.87636024],\n",
       "       [0.81793195],\n",
       "       [0.83307433],\n",
       "       [0.9468066 ],\n",
       "       [0.11039772],\n",
       "       [0.14624383],\n",
       "       [0.7250007 ],\n",
       "       [0.01512116],\n",
       "       [0.3344173 ],\n",
       "       [0.11829128],\n",
       "       [0.63206553],\n",
       "       [0.5879121 ],\n",
       "       [0.06061321],\n",
       "       [0.07178056],\n",
       "       [0.51276785],\n",
       "       [0.14154987],\n",
       "       [0.13133064],\n",
       "       [0.19818608],\n",
       "       [0.19890082],\n",
       "       [0.99904436],\n",
       "       [0.98178387],\n",
       "       [0.41765794],\n",
       "       [0.8705928 ],\n",
       "       [0.99214864],\n",
       "       [0.0014083 ],\n",
       "       [0.9720461 ],\n",
       "       [0.2429119 ],\n",
       "       [0.50838935],\n",
       "       [0.27825677],\n",
       "       [0.10985623],\n",
       "       [0.16610076],\n",
       "       [0.03055995],\n",
       "       [0.09013823],\n",
       "       [0.08575805],\n",
       "       [0.7936776 ],\n",
       "       [0.32516983],\n",
       "       [0.9939639 ],\n",
       "       [0.0544574 ],\n",
       "       [0.7370053 ],\n",
       "       [0.62771547],\n",
       "       [0.01689564],\n",
       "       [0.06262793],\n",
       "       [0.9734205 ],\n",
       "       [0.69926685],\n",
       "       [0.96912956],\n",
       "       [0.1322342 ],\n",
       "       [0.09488259],\n",
       "       [0.48329622],\n",
       "       [0.3724405 ],\n",
       "       [0.3569076 ],\n",
       "       [0.99214864],\n",
       "       [0.01454825],\n",
       "       [0.04412304],\n",
       "       [0.17898804],\n",
       "       [0.9971308 ],\n",
       "       [0.22471759],\n",
       "       [0.05052258],\n",
       "       [0.8203142 ],\n",
       "       [0.08821353],\n",
       "       [0.03408529],\n",
       "       [0.17920831],\n",
       "       [0.262174  ],\n",
       "       [0.15028085],\n",
       "       [0.39179358],\n",
       "       [0.44139045],\n",
       "       [0.20081861],\n",
       "       [0.1097021 ],\n",
       "       [0.42028898],\n",
       "       [0.02166042],\n",
       "       [0.9416761 ],\n",
       "       [0.8259603 ],\n",
       "       [0.48543748],\n",
       "       [0.03728373],\n",
       "       [0.02176815],\n",
       "       [0.9876159 ],\n",
       "       [0.69585156],\n",
       "       [0.99957305],\n",
       "       [0.2802806 ],\n",
       "       [0.8968764 ],\n",
       "       [0.13773583],\n",
       "       [0.6497945 ],\n",
       "       [0.7501389 ],\n",
       "       [0.02350063],\n",
       "       [0.98115736],\n",
       "       [0.11509303],\n",
       "       [0.5592044 ],\n",
       "       [0.998089  ],\n",
       "       [0.14336632],\n",
       "       [0.03526389],\n",
       "       [0.35521716],\n",
       "       [0.01251776],\n",
       "       [0.4840098 ],\n",
       "       [0.9998634 ],\n",
       "       [0.2935221 ],\n",
       "       [0.942575  ],\n",
       "       [0.26946405],\n",
       "       [0.7805728 ],\n",
       "       [0.25204435],\n",
       "       [0.3167793 ],\n",
       "       [0.01845385],\n",
       "       [0.6763371 ],\n",
       "       [0.01443235],\n",
       "       [0.17620471],\n",
       "       [0.9609415 ],\n",
       "       [0.94279003],\n",
       "       [0.99607915],\n",
       "       [0.8123775 ],\n",
       "       [0.04399337],\n",
       "       [0.3383249 ],\n",
       "       [0.01734649],\n",
       "       [0.50233704],\n",
       "       [0.36030722],\n",
       "       [0.8964937 ],\n",
       "       [0.0412337 ],\n",
       "       [0.79262793],\n",
       "       [0.84763914],\n",
       "       [0.3039556 ],\n",
       "       [0.21681333],\n",
       "       [0.25974065],\n",
       "       [0.20425025],\n",
       "       [0.40762827],\n",
       "       [0.65114516],\n",
       "       [0.9989411 ],\n",
       "       [0.05961887],\n",
       "       [0.00788963],\n",
       "       [0.01388317],\n",
       "       [0.25890273],\n",
       "       [0.23407653],\n",
       "       [0.0285531 ],\n",
       "       [0.8046263 ],\n",
       "       [0.11490876],\n",
       "       [0.22611223],\n",
       "       [0.2590255 ],\n",
       "       [0.24198708],\n",
       "       [0.9457433 ],\n",
       "       [0.19471341],\n",
       "       [0.605749  ],\n",
       "       [0.3635551 ],\n",
       "       [0.01511845],\n",
       "       [0.10692406],\n",
       "       [0.99971205],\n",
       "       [0.8302238 ],\n",
       "       [0.00445304],\n",
       "       [0.35667008],\n",
       "       [0.17265211],\n",
       "       [0.09877098],\n",
       "       [0.89841694],\n",
       "       [0.53785866],\n",
       "       [0.68148094],\n",
       "       [0.4004339 ],\n",
       "       [0.20820488],\n",
       "       [0.69167215],\n",
       "       [0.17533377],\n",
       "       [0.05949477],\n",
       "       [0.91913056],\n",
       "       [0.1824411 ],\n",
       "       [0.36746088],\n",
       "       [0.98037773],\n",
       "       [0.339079  ],\n",
       "       [0.39137083],\n",
       "       [0.00133892],\n",
       "       [0.29065326],\n",
       "       [0.8576309 ],\n",
       "       [0.99978   ],\n",
       "       [0.72868896],\n",
       "       [0.05096198],\n",
       "       [0.991743  ],\n",
       "       [0.4514265 ],\n",
       "       [0.88214666],\n",
       "       [0.4514265 ],\n",
       "       [0.99345195],\n",
       "       [0.01099837],\n",
       "       [0.4111997 ],\n",
       "       [0.11600288],\n",
       "       [0.979151  ],\n",
       "       [0.28155532],\n",
       "       [0.4920456 ],\n",
       "       [0.06735389],\n",
       "       [0.3309051 ],\n",
       "       [0.10532401],\n",
       "       [0.0211844 ],\n",
       "       [0.29305404],\n",
       "       [0.0812581 ],\n",
       "       [0.09272558],\n",
       "       [0.84377646],\n",
       "       [0.02421584],\n",
       "       [0.13510397],\n",
       "       [0.038552  ],\n",
       "       [0.01519149],\n",
       "       [0.12294699],\n",
       "       [0.68954134],\n",
       "       [0.07405496],\n",
       "       [0.67031425],\n",
       "       [0.15087028],\n",
       "       [0.20950185],\n",
       "       [0.9066033 ],\n",
       "       [0.15832557],\n",
       "       [0.06535428],\n",
       "       [0.06369495],\n",
       "       [0.00728166],\n",
       "       [0.9889177 ],\n",
       "       [0.01519149],\n",
       "       [0.40972796],\n",
       "       [0.99177235],\n",
       "       [0.99421096],\n",
       "       [0.9972324 ],\n",
       "       [0.9998939 ],\n",
       "       [0.99951947],\n",
       "       [0.28778908],\n",
       "       [0.09718145],\n",
       "       [0.2950511 ],\n",
       "       [0.6077685 ],\n",
       "       [0.9970541 ],\n",
       "       [0.52898234],\n",
       "       [0.27838036],\n",
       "       [0.60559773],\n",
       "       [0.75077415],\n",
       "       [0.02764558],\n",
       "       [0.00670361],\n",
       "       [0.11016101],\n",
       "       [0.38945332],\n",
       "       [0.00865706],\n",
       "       [0.06794166],\n",
       "       [0.43239266],\n",
       "       [0.9911248 ],\n",
       "       [0.02728077],\n",
       "       [0.9239783 ],\n",
       "       [0.7346232 ],\n",
       "       [0.07780352],\n",
       "       [0.30433115],\n",
       "       [0.12580794],\n",
       "       [0.7585295 ],\n",
       "       [0.49075863],\n",
       "       [0.00970201]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43768293], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at a single prediction\n",
    "model_1_pred_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43768293],\n",
       "       [0.75415933],\n",
       "       [0.9975879 ],\n",
       "       [0.14311554],\n",
       "       [0.13520585],\n",
       "       [0.9465931 ],\n",
       "       [0.9103034 ],\n",
       "       [0.9930292 ],\n",
       "       [0.9723433 ],\n",
       "       [0.313366  ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first 10\n",
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model prediction probabilities to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.08398950131233,\n",
       " 'precision': 0.7823342077218286,\n",
       " 'recall': 0.7808398950131233,\n",
       " 'f1-score': 0.7790338643605079}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate our model_1 results\n",
    "model_1_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1-score': 0.7862189758049549}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing learned embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorization layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 1 summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00621972 -0.01455345 -0.04495444 ... -0.02000687 -0.06168159\n",
      "   0.02624471]\n",
      " [-0.01366551 -0.02133484 -0.01971176 ...  0.02678423 -0.03069613\n",
      "   0.04313565]\n",
      " [ 0.06011938 -0.02059139 -0.04062791 ...  0.02113723  0.03200927\n",
      "  -0.02893194]\n",
      " ...\n",
      " [ 0.02577816  0.03853223  0.00585351 ... -0.03576713 -0.02949489\n",
      "  -0.0173419 ]\n",
      " [-0.00841635  0.00502448 -0.04013104 ... -0.08449718  0.00413191\n",
      "   0.00630612]\n",
      " [ 0.03933034 -0.03421945 -0.06514011 ... -0.06822641 -0.05384304\n",
      "   0.00815871]]\n",
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Get the weight matrix of embedding layer\n",
    "# (these are the numerical representations of each token in our training data, which have been learned for 5 epochs)\n",
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()[0]\n",
    "print(embed_weights)\n",
    "print(embed_weights.shape) # same size as vocab size and embedding_dim (output_dim of our embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got the embedding matrix our model has learned to represent our tokens, let's see how we can visualize it.\n",
    "\n",
    "To do so, TensorFlow has a handy tool called projector: http://projector.tensorflow.org/\n",
    "\n",
    "And TensorFlow also has an incredible guide on word embeddings themselves: https://www.tensorflow.org/text/guide/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding files (we got this from TensorFlow's word embeddings documentation)\n",
    "import io\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = embed_weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the files above we can visualize them using http://projector.tensorflow.org/ and clicking the \"load\" button on  the left hand side.\n",
    "\n",
    "> üìñ **Ressource:** If you'd like to know more about embeddings, I'd encourage you to check out:\n",
    "* Jay Alammar's visualized word2vec post: https://jalammar.github.io/illustrated-word2vec/\n",
    "* TensorFlow's Word Embeddings guide: https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNN's are useful for sequence data.\n",
    "\n",
    "The premise of a recurrent neural network is to use the representation of a previous input to aid the representation of a later input.\n",
    "\n",
    "> üìñ **Ressources:** If you want an overview of the internals of a recurrent neural network, see the following:\n",
    " - MIT's sequence modell lecture: https://youtu.be/qjrad0V0uJE\n",
    " - Chris Olag's intro to LSTMs: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    " - Andrej Kaparthy's the unreasonable effectiveness of recurrent neural networks: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LSTM\n",
    "LSTM  = long short term memory (one of the most popular LSTM cells)\n",
    "\n",
    "Our structure of an RNN typically looks like this:\n",
    "```Input(text) -> Tokenize -> Embedding -> Layers (RNNs/dense) -> Output (label probability)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LSTM model\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1, ), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "#x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.LSTM(64)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,333,633\n",
      "Trainable params: 1,333,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the nmodel\n",
    "model_2.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_2_LSTM/20220712-130559\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 15ms/step - loss: 0.2247 - accuracy: 0.9171 - val_loss: 0.6255 - val_accuracy: 0.7664\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.1546 - accuracy: 0.9431 - val_loss: 0.6062 - val_accuracy: 0.7743\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1296 - accuracy: 0.9514 - val_loss: 0.7270 - val_accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.1042 - accuracy: 0.9593 - val_loss: 0.8789 - val_accuracy: 0.7782\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0857 - accuracy: 0.9670 - val_loss: 1.0245 - val_accuracy: 0.7690\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "histor_model_2 = model_2.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        SAVE_DIR,\n",
    "        \"model_2_LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0700102e-02],\n",
       "       [7.3110503e-01],\n",
       "       [9.9987841e-01],\n",
       "       [1.8816775e-02],\n",
       "       [4.3916469e-05],\n",
       "       [9.9444902e-01],\n",
       "       [8.5322934e-01],\n",
       "       [9.9993443e-01],\n",
       "       [9.9982315e-01],\n",
       "       [3.3262169e-01]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with LSTM model\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model 2 pred probs to labels\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.9028871391076,\n",
       " 'precision': 0.7706028054440214,\n",
       " 'recall': 0.7690288713910761,\n",
       " 'f1-score': 0.7669342344352704}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 2 results\n",
    "model_2_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1-score': 0.7862189758049549}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: GRU\n",
    "\n",
    "Another popular and effective component is the GRU or gated recurrent unit.\n",
    "\n",
    "The GRU cell has similar features to an LSTM cell but has less parameters.\n",
    "\n",
    "> üìñ **Ressource:** \n",
    " - https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
    " - https://en.wikipedia.org/wiki/Gated_recurrent_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an RNN using the GRU cell\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.GRU(64, return_sequences=True)(x) # if you wand to stack recurrent layers --> return_sequences = True\n",
    "# x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.GRU(64)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,321,473\n",
      "Trainable params: 1,321,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_3.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_3_GRU/20220712-130615\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 4s 14ms/step - loss: 0.1534 - accuracy: 0.9403 - val_loss: 0.7090 - val_accuracy: 0.7769\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0807 - accuracy: 0.9699 - val_loss: 0.9499 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0691 - accuracy: 0.9727 - val_loss: 0.9026 - val_accuracy: 0.7756\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.0591 - accuracy: 0.9765 - val_loss: 1.3017 - val_accuracy: 0.7664\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.0574 - accuracy: 0.9749 - val_loss: 1.2652 - val_accuracy: 0.7782\n"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "model_3_history = model_3.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        SAVE_DIR,\n",
    "        \"model_3_GRU\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0657791e-03],\n",
       "       [5.8213645e-01],\n",
       "       [9.9999285e-01],\n",
       "       [4.9950864e-02],\n",
       "       [4.0438168e-05],\n",
       "       [9.9988580e-01],\n",
       "       [8.2895452e-01],\n",
       "       [9.9999809e-01],\n",
       "       [9.9999499e-01],\n",
       "       [9.8485237e-01]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pred_probs to labels\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.82152230971128,\n",
       " 'precision': 0.7814103276314137,\n",
       " 'recall': 0.7782152230971129,\n",
       " 'f1-score': 0.7756075024838144}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 3 results\n",
    "model_3_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Bidirectional RNN\n",
    "\n",
    "Normal RNN's go from left to right (just like you'd read an English sentence), however a bidirectional RNN goes from right to left as well as left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìñ **Ressources:** \n",
    " - https://easy-tensorflow.com/tf-tutorials/recurrent-neural-networks/bidirectional-rnn-for-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a bidirectional RNN (BRNN)\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_BRNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_BRNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_4_BRNN/20220712-130628\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 16ms/step - loss: 0.1095 - accuracy: 0.9661 - val_loss: 0.9299 - val_accuracy: 0.7690\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.0568 - accuracy: 0.9752 - val_loss: 1.2174 - val_accuracy: 0.7690\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.0471 - accuracy: 0.9784 - val_loss: 1.2062 - val_accuracy: 0.7598\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.0428 - accuracy: 0.9797 - val_loss: 1.3952 - val_accuracy: 0.7677\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.0410 - accuracy: 0.9810 - val_loss: 1.4186 - val_accuracy: 0.7546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16dc4efb520>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_4.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        SAVE_DIR,\n",
    "        \"model_4_BRNN\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5679765e-02],\n",
       "       [9.1660917e-01],\n",
       "       [9.9998009e-01],\n",
       "       [1.8723889e-01],\n",
       "       [2.2006963e-05],\n",
       "       [9.9934703e-01],\n",
       "       [9.2616528e-01],\n",
       "       [9.9999130e-01],\n",
       "       [9.9998450e-01],\n",
       "       [9.9508590e-01]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.45931758530183,\n",
       " 'precision': 0.7542299722656125,\n",
       " 'recall': 0.7545931758530183,\n",
       " 'f1-score': 0.7537160541571394}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_4_preds\n",
    ")\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks for Text (and other types of sequences)\n",
    "\n",
    "We've used CNNs for images but images are typically 2D (height x width)... however our textdata is 1D.\n",
    "\n",
    "Previously we've used Conv2D for our image data but now we're going to use Conv1D.\n",
    "\n",
    "The typical structure of a Conv1D model for sequences (in our case, text):\n",
    "```\n",
    "Inputs (text) -> Tokenization -> Embedding -> Layer(s) (typically Conv1D + pooling) -> Outputs (class probabilities)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Conv1D\n",
    "\n",
    "For different explanations of parameters see:\n",
    "* https://poloclub.github.io/cnn-explainer/ (this is for 2D but can relate to 1D data)\n",
    "* Difference between same and valid padding: https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 15, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out our embedding layer, Conv1D layer and max pooling\n",
    "from tensorflow.keras import layers\n",
    "embedding_test = embedding(text_vectorizer([\"This a test sentence\"])) # turn target sequence into an embedding\n",
    "conv_1d = layers.Conv1D(\n",
    "    filters=32,\n",
    "    kernel_size=5,\n",
    "    activation=\"relu\",\n",
    "    padding=\"same\"\n",
    ")\n",
    "conv_1d_output = conv_1d(embedding_test) #6 pass test embedding through our Conv1D layer\n",
    "max_pool = layers.GlobalMaxPooling1D()\n",
    "max_pool_output = max_pool(conv_1d_output) # equivalent to \"get the most important feature\" or \"get the feature with the highest value\"\n",
    "\n",
    "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.01829151, -0.0337957 , -0.04516763, ..., -0.04508371,\n",
       "         -0.02727526,  0.02372266],\n",
       "        [ 0.03785113,  0.01064004, -0.02305558, ..., -0.04116302,\n",
       "          0.05203898,  0.03469247],\n",
       "        [ 0.00659516,  0.01812489, -0.03870727, ..., -0.00069144,\n",
       "         -0.04381865,  0.02243287],\n",
       "        ...,\n",
       "        [ 0.01708225,  0.00111116, -0.03085442, ..., -0.0213538 ,\n",
       "         -0.01436895,  0.01179904],\n",
       "        [ 0.01708225,  0.00111116, -0.03085442, ..., -0.0213538 ,\n",
       "         -0.01436895,  0.01179904],\n",
       "        [ 0.01708225,  0.00111116, -0.03085442, ..., -0.0213538 ,\n",
       "         -0.01436895,  0.01179904]]], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 32), dtype=float32, numpy=\n",
       "array([[[0.00000000e+00, 2.94077042e-02, 0.00000000e+00, 2.26540044e-02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.98353222e-02,\n",
       "         9.36238468e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 6.81192279e-02, 1.57576287e-05, 0.00000000e+00,\n",
       "         7.69947469e-03, 0.00000000e+00, 0.00000000e+00, 8.81991982e-02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.89199068e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 5.41345403e-02, 5.29715791e-03],\n",
       "        [0.00000000e+00, 4.51640040e-02, 2.70973556e-02, 2.03636438e-02,\n",
       "         8.25588999e-04, 1.94355398e-02, 1.71626322e-02, 1.07317992e-01,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.38987112e-02, 7.76557475e-02,\n",
       "         3.13858315e-02, 8.81493650e-03, 3.35314944e-02, 0.00000000e+00,\n",
       "         4.37760055e-02, 6.95857480e-02, 7.15725645e-02, 1.86460111e-02,\n",
       "         4.79563996e-02, 2.40903161e-02, 4.32982780e-02, 3.09326639e-03,\n",
       "         0.00000000e+00, 6.18090760e-03, 0.00000000e+00, 1.11444769e-02,\n",
       "         2.95007098e-02, 1.37215201e-02, 0.00000000e+00, 2.99975909e-02],\n",
       "        [0.00000000e+00, 2.43177568e-03, 5.28198592e-02, 3.86353396e-02,\n",
       "         0.00000000e+00, 2.39947252e-03, 0.00000000e+00, 3.01413648e-02,\n",
       "         0.00000000e+00, 3.72129828e-02, 0.00000000e+00, 7.65691996e-02,\n",
       "         1.49054499e-02, 6.67637438e-02, 2.61841416e-02, 0.00000000e+00,\n",
       "         8.92349519e-03, 0.00000000e+00, 0.00000000e+00, 2.82093138e-02,\n",
       "         3.38133797e-02, 3.33219841e-02, 4.14196439e-02, 9.73744877e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         2.45927498e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 6.25422895e-02, 0.00000000e+00, 2.57424992e-02,\n",
       "         0.00000000e+00, 0.00000000e+00, 6.74993470e-02, 1.08603109e-03,\n",
       "         2.15457287e-02, 0.00000000e+00, 0.00000000e+00, 6.39205724e-02,\n",
       "         0.00000000e+00, 1.40472800e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.89235061e-02, 0.00000000e+00, 1.53094679e-02,\n",
       "         0.00000000e+00, 1.71130821e-02, 2.67161094e-02, 5.78661114e-02,\n",
       "         7.60836005e-02, 9.63519141e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         2.16071308e-02, 4.09907021e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.78102374e-02,\n",
       "         0.00000000e+00, 1.48930121e-04, 3.80800106e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.67811404e-03,\n",
       "         1.93049107e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.20179856e-02, 5.57923988e-02, 1.62989944e-02,\n",
       "         0.00000000e+00, 1.26665505e-02, 4.79636677e-02, 2.08968762e-03,\n",
       "         0.00000000e+00, 2.58215629e-02, 8.60945359e-02, 0.00000000e+00,\n",
       "         2.23983899e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 6.35480285e-02, 1.24748610e-02, 3.88640761e-02,\n",
       "         2.07039504e-03, 0.00000000e+00, 8.57677013e-02, 0.00000000e+00,\n",
       "         9.95464437e-03, 0.00000000e+00, 0.00000000e+00, 2.72504482e-02,\n",
       "         0.00000000e+00, 4.06816751e-02, 2.21857354e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.04545557e-02, 3.12566832e-02, 5.12593333e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 8.08128715e-03, 2.22483650e-04,\n",
       "         0.00000000e+00, 4.02418664e-04, 1.81198958e-03, 5.91636170e-04,\n",
       "         5.50417751e-02, 1.32981781e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.42973810e-02, 0.00000000e+00, 5.16736787e-03,\n",
       "         0.00000000e+00, 1.46362884e-02, 3.90886292e-02, 9.48944595e-03,\n",
       "         1.70698632e-02, 0.00000000e+00, 0.00000000e+00, 1.89056545e-02,\n",
       "         0.00000000e+00, 1.87755823e-02, 1.98649578e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.75816750e-02, 2.73478590e-02, 6.33827690e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.28586139e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.73835047e-02, 2.77301557e-02,\n",
       "         4.14501056e-02, 3.21062654e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.42973969e-02, 0.00000000e+00, 5.16736042e-03,\n",
       "         0.00000000e+00, 1.46362819e-02, 3.90886292e-02, 9.48944874e-03,\n",
       "         1.70698725e-02, 0.00000000e+00, 0.00000000e+00, 1.89056583e-02,\n",
       "         0.00000000e+00, 1.87755898e-02, 1.98649596e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.75816824e-02, 2.73478590e-02, 6.33827783e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.28586176e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.73834935e-02, 2.77301650e-02,\n",
       "         4.14501056e-02, 3.21062729e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.42973959e-02, 0.00000000e+00, 5.16735855e-03,\n",
       "         0.00000000e+00, 1.46362837e-02, 3.90886292e-02, 9.48945154e-03,\n",
       "         1.70698650e-02, 0.00000000e+00, 0.00000000e+00, 1.89056601e-02,\n",
       "         0.00000000e+00, 1.87755860e-02, 1.98649615e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.75816880e-02, 2.73478627e-02, 6.33827597e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.28586213e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.73834860e-02, 2.77301688e-02,\n",
       "         4.14501056e-02, 3.21062766e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.42973950e-02, 0.00000000e+00, 5.16736135e-03,\n",
       "         0.00000000e+00, 1.46362856e-02, 3.90886292e-02, 9.48944967e-03,\n",
       "         1.70698650e-02, 0.00000000e+00, 0.00000000e+00, 1.89056676e-02,\n",
       "         0.00000000e+00, 1.87755935e-02, 1.98649596e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.75816899e-02, 2.73478664e-02, 6.33827550e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.28586251e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.73834823e-02, 2.77301669e-02,\n",
       "         4.14501131e-02, 3.21062803e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.42973922e-02, 0.00000000e+00, 5.16735576e-03,\n",
       "         0.00000000e+00, 1.46362856e-02, 3.90886217e-02, 9.48945247e-03,\n",
       "         1.70698669e-02, 0.00000000e+00, 0.00000000e+00, 1.89056639e-02,\n",
       "         0.00000000e+00, 1.87755935e-02, 1.98649690e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.75816880e-02, 2.73478590e-02, 6.33827411e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.28586176e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.73834823e-02, 2.77301781e-02,\n",
       "         4.14501131e-02, 3.21062766e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.42973959e-02, 0.00000000e+00, 5.16735809e-03,\n",
       "         0.00000000e+00, 1.46362828e-02, 3.90886255e-02, 9.48945247e-03,\n",
       "         1.70698650e-02, 0.00000000e+00, 0.00000000e+00, 1.89056620e-02,\n",
       "         0.00000000e+00, 1.87755916e-02, 1.98649596e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.75816843e-02, 2.73478590e-02, 6.33827224e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.28586176e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.73834860e-02, 2.77301706e-02,\n",
       "         4.14501131e-02, 3.21062729e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.42973876e-02, 0.00000000e+00, 5.16736414e-03,\n",
       "         0.00000000e+00, 1.46362819e-02, 3.90886255e-02, 9.48944595e-03,\n",
       "         1.70698650e-02, 0.00000000e+00, 0.00000000e+00, 1.89056620e-02,\n",
       "         0.00000000e+00, 1.87755898e-02, 1.98649578e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.75816880e-02, 2.73478627e-02, 6.33827131e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.28586213e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.73834916e-02, 2.77301669e-02,\n",
       "         4.14501056e-02, 3.21062729e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 5.49466815e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 7.65495421e-03, 2.83749700e-02, 2.01214701e-02,\n",
       "         9.02266055e-03, 0.00000000e+00, 0.00000000e+00, 2.06768569e-02,\n",
       "         0.00000000e+00, 1.90639794e-02, 1.19604515e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.38183253e-02, 4.44985852e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 2.39736345e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.73002291e-02,\n",
       "         6.45762309e-02, 4.80059236e-02, 0.00000000e+00, 1.61447711e-02],\n",
       "        [0.00000000e+00, 1.04621835e-02, 8.06012098e-03, 0.00000000e+00,\n",
       "         2.07728576e-02, 7.00513646e-03, 7.32802181e-03, 2.64112912e-02,\n",
       "         2.25569792e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.36475321e-02, 2.45498978e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.39051303e-02, 3.32924388e-02, 1.96416862e-04,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.11729680e-02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 3.24336812e-03, 4.55734804e-02,\n",
       "         1.66139863e-02, 3.15582976e-02, 0.00000000e+00, 1.98063776e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
       "array([[0.        , 0.06354803, 0.05281986, 0.06781024, 0.02077286,\n",
       "        0.01943554, 0.0857677 , 0.10731799, 0.02255698, 0.03721298,\n",
       "        0.03389871, 0.07765575, 0.03138583, 0.06676374, 0.03353149,\n",
       "        0.        , 0.04377601, 0.06958575, 0.07157256, 0.02820931,\n",
       "        0.0479564 , 0.03332198, 0.04796367, 0.0881992 , 0.0760836 ,\n",
       "        0.02582156, 0.08609454, 0.04557348, 0.06457623, 0.04800592,\n",
       "        0.05413454, 0.02999759]], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pool_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1-d convolutional layer to model sequences\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(\n",
    "    filters=64, \n",
    "    kernel_size=5, # each filter is looking at 5 words at a time\n",
    "    strides=1,\n",
    "    activation=\"relu\",\n",
    "    padding=\"valid\",\n",
    ")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "# x =layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 11, 64)            41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,321,089\n",
      "Trainable params: 1,321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_5_Conv1D/20220712-130645\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 9ms/step - loss: 0.1239 - accuracy: 0.9591 - val_loss: 0.9505 - val_accuracy: 0.7677\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0745 - accuracy: 0.9723 - val_loss: 1.0531 - val_accuracy: 0.7585\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0594 - accuracy: 0.9769 - val_loss: 1.1608 - val_accuracy: 0.7651\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.0539 - accuracy: 0.9787 - val_loss: 1.1835 - val_accuracy: 0.7717\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0517 - accuracy: 0.9778 - val_loss: 1.2467 - val_accuracy: 0.7585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16dc4dbdf40>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        SAVE_DIR,\n",
    "        \"model_5_Conv1D\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.3751248e-02],\n",
       "       [9.4955790e-01],\n",
       "       [9.9986589e-01],\n",
       "       [4.1823719e-02],\n",
       "       [2.5007859e-07],\n",
       "       [9.8249120e-01],\n",
       "       [8.1763923e-01],\n",
       "       [9.9994946e-01],\n",
       "       [9.9999881e-01],\n",
       "       [9.1972202e-01]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.8530183727034,\n",
       " 'precision': 0.7590334269164114,\n",
       " 'recall': 0.7585301837270341,\n",
       " 'f1-score': 0.7568554876539897}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_5_preds\n",
    ")\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: TensorFlow Hub Pretrained Sentence Encoder\n",
    "\n",
    "Now we've build a few of our own model, let's try and use transfer leraning for NLP, specifically using TensorFlow Hub's Universal Sentence Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There's a flood in my street\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01154495  0.02487101  0.02879629 -0.01272263  0.0396995   0.08829076\n",
      "  0.02682647  0.05582222 -0.01078761 -0.00596656  0.00640639 -0.01816132\n",
      "  0.0002885   0.09106605  0.05874373 -0.03175148  0.01510154 -0.05164852\n",
      "  0.00994341 -0.06867752 -0.04210395  0.02675389  0.03008907  0.00320448\n",
      " -0.00336864 -0.0479053   0.02267517 -0.00984555 -0.04066692 -0.01285528\n",
      " -0.04665243  0.05630673 -0.03952145  0.00521895  0.02495947 -0.07011834\n",
      "  0.02873132  0.04945794 -0.00634556 -0.08959357  0.02807156 -0.00809173\n",
      " -0.01363955  0.05998397 -0.10361549 -0.05192674  0.0023246  -0.0232653\n",
      " -0.03752431  0.03332979], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras Layer using the USW pretrained layer from tensorflow hub\n",
    "sentence_encoder_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "    input_shape=[],\n",
    "    dtype=tf.string,\n",
    "    trainable=False,\n",
    "    name=\"USE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model using the Sequential API\n",
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\", name=\"output_layer\"),\n",
    "], name=\"model_6_USE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_6.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "USE (KerasLayer)             (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20220712-130708\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 13s 50ms/step - loss: 0.5029 - accuracy: 0.7892 - val_loss: 0.4532 - val_accuracy: 0.7966\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4154 - accuracy: 0.8136 - val_loss: 0.4389 - val_accuracy: 0.8071\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.4024 - accuracy: 0.8218 - val_loss: 0.4337 - val_accuracy: 0.8189\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.3946 - accuracy: 0.8270 - val_loss: 0.4329 - val_accuracy: 0.8071\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 14ms/step - loss: 0.3888 - accuracy: 0.8269 - val_loss: 0.4281 - val_accuracy: 0.8123\n"
     ]
    }
   ],
   "source": [
    "# Train a classifier on top of USE pretrained embeddings\n",
    "model_6_history = model_6.fit(\n",
    "    train_sentences,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        SAVE_DIR,\n",
    "        \"tf_hub_sentence_encoder\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.2036399 ],\n",
       "        [0.8039708 ],\n",
       "        [0.9877743 ],\n",
       "        [0.19731353],\n",
       "        [0.77383107],\n",
       "        [0.77808386],\n",
       "        [0.9814402 ],\n",
       "        [0.97966206],\n",
       "        [0.95053667],\n",
       "        [0.09393004],\n",
       "        [0.70325214],\n",
       "        [0.5436467 ],\n",
       "        [0.19717072],\n",
       "        [0.5581286 ],\n",
       "        [0.3151944 ],\n",
       "        [0.02684455],\n",
       "        [0.41514835],\n",
       "        [0.6591518 ],\n",
       "        [0.43635848],\n",
       "        [0.37142685],\n",
       "        [0.9167749 ],\n",
       "        [0.15431406],\n",
       "        [0.5238324 ],\n",
       "        [0.03080136],\n",
       "        [0.8901003 ],\n",
       "        [0.9688959 ],\n",
       "        [0.06223309],\n",
       "        [0.10755428],\n",
       "        [0.10748728],\n",
       "        [0.36467654],\n",
       "        [0.5665341 ],\n",
       "        [0.91793835],\n",
       "        [0.41082326],\n",
       "        [0.357929  ],\n",
       "        [0.56579995],\n",
       "        [0.10537849],\n",
       "        [0.98100203],\n",
       "        [0.05338581],\n",
       "        [0.03358223],\n",
       "        [0.9839208 ],\n",
       "        [0.08181841],\n",
       "        [0.30985355],\n",
       "        [0.5305946 ],\n",
       "        [0.5515532 ],\n",
       "        [0.2823618 ],\n",
       "        [0.9624601 ],\n",
       "        [0.41937914],\n",
       "        [0.9629545 ],\n",
       "        [0.7053424 ],\n",
       "        [0.8499161 ],\n",
       "        [0.05214759],\n",
       "        [0.72436357],\n",
       "        [0.20010103],\n",
       "        [0.05646645],\n",
       "        [0.10917258],\n",
       "        [0.03214002],\n",
       "        [0.13277943],\n",
       "        [0.9055118 ],\n",
       "        [0.12877215],\n",
       "        [0.05424468],\n",
       "        [0.08083185],\n",
       "        [0.9792342 ],\n",
       "        [0.97894764],\n",
       "        [0.09357834],\n",
       "        [0.9252213 ],\n",
       "        [0.96232975],\n",
       "        [0.7312247 ],\n",
       "        [0.2000491 ],\n",
       "        [0.06435082],\n",
       "        [0.55526286],\n",
       "        [0.1106726 ],\n",
       "        [0.07624998],\n",
       "        [0.791463  ],\n",
       "        [0.05097756],\n",
       "        [0.03018795],\n",
       "        [0.4976785 ],\n",
       "        [0.14113486],\n",
       "        [0.28795436],\n",
       "        [0.06765423],\n",
       "        [0.7776493 ],\n",
       "        [0.77247125],\n",
       "        [0.52254957],\n",
       "        [0.94476074],\n",
       "        [0.09361628],\n",
       "        [0.70371664],\n",
       "        [0.63881296],\n",
       "        [0.08187323],\n",
       "        [0.13912235],\n",
       "        [0.98555434],\n",
       "        [0.9046879 ],\n",
       "        [0.9957866 ],\n",
       "        [0.03831686],\n",
       "        [0.09304617],\n",
       "        [0.02585443],\n",
       "        [0.95981354],\n",
       "        [0.7797892 ],\n",
       "        [0.9728731 ],\n",
       "        [0.8971086 ],\n",
       "        [0.97794   ],\n",
       "        [0.9306424 ],\n",
       "        [0.94687545],\n",
       "        [0.16726837],\n",
       "        [0.1849593 ],\n",
       "        [0.9885799 ],\n",
       "        [0.9762631 ],\n",
       "        [0.03562982],\n",
       "        [0.89206624],\n",
       "        [0.8142561 ],\n",
       "        [0.15572068],\n",
       "        [0.8051429 ],\n",
       "        [0.17218722],\n",
       "        [0.05459536],\n",
       "        [0.28257158],\n",
       "        [0.3203422 ],\n",
       "        [0.21751887],\n",
       "        [0.2258814 ],\n",
       "        [0.5447946 ],\n",
       "        [0.67801774],\n",
       "        [0.7920863 ],\n",
       "        [0.7697315 ],\n",
       "        [0.9395752 ],\n",
       "        [0.14047363],\n",
       "        [0.6566961 ],\n",
       "        [0.8387403 ],\n",
       "        [0.5849668 ],\n",
       "        [0.43696228],\n",
       "        [0.18305725],\n",
       "        [0.11765915],\n",
       "        [0.12815212],\n",
       "        [0.39861476],\n",
       "        [0.1072249 ],\n",
       "        [0.953528  ],\n",
       "        [0.96762675],\n",
       "        [0.969349  ],\n",
       "        [0.81212866],\n",
       "        [0.27573588],\n",
       "        [0.9644882 ],\n",
       "        [0.4818622 ],\n",
       "        [0.5215759 ],\n",
       "        [0.1322254 ],\n",
       "        [0.97724414],\n",
       "        [0.14219077],\n",
       "        [0.53978354],\n",
       "        [0.5000001 ],\n",
       "        [0.78378165],\n",
       "        [0.4572282 ],\n",
       "        [0.03253744],\n",
       "        [0.07778817],\n",
       "        [0.1584359 ],\n",
       "        [0.91349506],\n",
       "        [0.41390845],\n",
       "        [0.05670056],\n",
       "        [0.5718504 ],\n",
       "        [0.06669736],\n",
       "        [0.47976685],\n",
       "        [0.8450755 ],\n",
       "        [0.69408625],\n",
       "        [0.07342106],\n",
       "        [0.94931084],\n",
       "        [0.18182294],\n",
       "        [0.91179425],\n",
       "        [0.08650742],\n",
       "        [0.18684778],\n",
       "        [0.9468997 ],\n",
       "        [0.07098032],\n",
       "        [0.05047364],\n",
       "        [0.9932273 ],\n",
       "        [0.17650269],\n",
       "        [0.9256915 ],\n",
       "        [0.15237255],\n",
       "        [0.98451555],\n",
       "        [0.5165037 ],\n",
       "        [0.97054255],\n",
       "        [0.06140279],\n",
       "        [0.95525587],\n",
       "        [0.03462437],\n",
       "        [0.86906177],\n",
       "        [0.5477807 ],\n",
       "        [0.63595486],\n",
       "        [0.9931144 ],\n",
       "        [0.04988956],\n",
       "        [0.6998812 ],\n",
       "        [0.46604776],\n",
       "        [0.88407356],\n",
       "        [0.92876416],\n",
       "        [0.33311886],\n",
       "        [0.17514063],\n",
       "        [0.9768027 ],\n",
       "        [0.36423814],\n",
       "        [0.08677721],\n",
       "        [0.3862594 ],\n",
       "        [0.9628462 ],\n",
       "        [0.08837792],\n",
       "        [0.20071205],\n",
       "        [0.10980073],\n",
       "        [0.08427803],\n",
       "        [0.10958842],\n",
       "        [0.24064808],\n",
       "        [0.08336659],\n",
       "        [0.06432445],\n",
       "        [0.1637519 ],\n",
       "        [0.95762604],\n",
       "        [0.8683392 ],\n",
       "        [0.25523737],\n",
       "        [0.11812843],\n",
       "        [0.98411524],\n",
       "        [0.33126995],\n",
       "        [0.8479755 ],\n",
       "        [0.92485523],\n",
       "        [0.8550885 ],\n",
       "        [0.03691059],\n",
       "        [0.97554725],\n",
       "        [0.05922789],\n",
       "        [0.12310153],\n",
       "        [0.03193202],\n",
       "        [0.04494832],\n",
       "        [0.915906  ],\n",
       "        [0.9256782 ],\n",
       "        [0.69103855],\n",
       "        [0.05768338],\n",
       "        [0.6035587 ],\n",
       "        [0.0595175 ],\n",
       "        [0.09462924],\n",
       "        [0.21989806],\n",
       "        [0.9872313 ],\n",
       "        [0.14171381],\n",
       "        [0.23508474],\n",
       "        [0.9601054 ],\n",
       "        [0.7234875 ],\n",
       "        [0.35608253],\n",
       "        [0.83013165],\n",
       "        [0.17346157],\n",
       "        [0.9445415 ],\n",
       "        [0.0498645 ],\n",
       "        [0.41403052],\n",
       "        [0.5426449 ],\n",
       "        [0.4464196 ],\n",
       "        [0.6115986 ],\n",
       "        [0.979813  ],\n",
       "        [0.03966869],\n",
       "        [0.6738985 ],\n",
       "        [0.28836268],\n",
       "        [0.9762496 ],\n",
       "        [0.9376571 ],\n",
       "        [0.03982302],\n",
       "        [0.2245783 ],\n",
       "        [0.9299982 ],\n",
       "        [0.21007033],\n",
       "        [0.10376202],\n",
       "        [0.77482057],\n",
       "        [0.12513451],\n",
       "        [0.8533752 ],\n",
       "        [0.03568004],\n",
       "        [0.61557925],\n",
       "        [0.8920774 ],\n",
       "        [0.20372747],\n",
       "        [0.80249655],\n",
       "        [0.99232423],\n",
       "        [0.3881176 ],\n",
       "        [0.14256921],\n",
       "        [0.9534132 ],\n",
       "        [0.09505045],\n",
       "        [0.1785718 ],\n",
       "        [0.9573541 ],\n",
       "        [0.89002526],\n",
       "        [0.73124576],\n",
       "        [0.92319596],\n",
       "        [0.06979019],\n",
       "        [0.16013156],\n",
       "        [0.07522219],\n",
       "        [0.14398426],\n",
       "        [0.43285576],\n",
       "        [0.9295287 ],\n",
       "        [0.11698879],\n",
       "        [0.4372691 ],\n",
       "        [0.93535817],\n",
       "        [0.17906836],\n",
       "        [0.07933725],\n",
       "        [0.9787676 ],\n",
       "        [0.51627666],\n",
       "        [0.10412459],\n",
       "        [0.14607033],\n",
       "        [0.9703001 ],\n",
       "        [0.40939036],\n",
       "        [0.1946329 ],\n",
       "        [0.6552    ],\n",
       "        [0.78421324],\n",
       "        [0.18474916],\n",
       "        [0.78993595],\n",
       "        [0.04181373],\n",
       "        [0.804619  ],\n",
       "        [0.5575182 ],\n",
       "        [0.30733997],\n",
       "        [0.7709108 ],\n",
       "        [0.07290465],\n",
       "        [0.89438105],\n",
       "        [0.28601015],\n",
       "        [0.9825051 ],\n",
       "        [0.07087635],\n",
       "        [0.5724635 ],\n",
       "        [0.41743258],\n",
       "        [0.11647987],\n",
       "        [0.07262758],\n",
       "        [0.7458546 ],\n",
       "        [0.2499241 ],\n",
       "        [0.19497675],\n",
       "        [0.09856556],\n",
       "        [0.74407136],\n",
       "        [0.04862347],\n",
       "        [0.02762984],\n",
       "        [0.12442851],\n",
       "        [0.9643877 ],\n",
       "        [0.7167137 ],\n",
       "        [0.25925472],\n",
       "        [0.9731627 ],\n",
       "        [0.06843397],\n",
       "        [0.91817474],\n",
       "        [0.6725228 ],\n",
       "        [0.0230715 ],\n",
       "        [0.27320373],\n",
       "        [0.08011009],\n",
       "        [0.15114853],\n",
       "        [0.9754936 ],\n",
       "        [0.06778232],\n",
       "        [0.77793425],\n",
       "        [0.06907689],\n",
       "        [0.21189496],\n",
       "        [0.8830943 ],\n",
       "        [0.0587669 ],\n",
       "        [0.88698196],\n",
       "        [0.8456658 ],\n",
       "        [0.10463978],\n",
       "        [0.9420363 ],\n",
       "        [0.26070243],\n",
       "        [0.19031769],\n",
       "        [0.60114914],\n",
       "        [0.04940936],\n",
       "        [0.42835894],\n",
       "        [0.5898637 ],\n",
       "        [0.59515905],\n",
       "        [0.10154407],\n",
       "        [0.17851749],\n",
       "        [0.93548936],\n",
       "        [0.968579  ],\n",
       "        [0.7591701 ],\n",
       "        [0.21144868],\n",
       "        [0.30851886],\n",
       "        [0.08846755],\n",
       "        [0.06478211],\n",
       "        [0.1930307 ],\n",
       "        [0.20556965],\n",
       "        [0.4475216 ],\n",
       "        [0.08123245],\n",
       "        [0.08929956],\n",
       "        [0.5762304 ],\n",
       "        [0.04055994],\n",
       "        [0.96485025],\n",
       "        [0.96141726],\n",
       "        [0.84005517],\n",
       "        [0.7217414 ],\n",
       "        [0.5047592 ],\n",
       "        [0.08018089],\n",
       "        [0.95861554],\n",
       "        [0.5691149 ],\n",
       "        [0.4681215 ],\n",
       "        [0.05295063],\n",
       "        [0.16265868],\n",
       "        [0.07403232],\n",
       "        [0.197697  ],\n",
       "        [0.02895859],\n",
       "        [0.8230299 ],\n",
       "        [0.03202073],\n",
       "        [0.13765438],\n",
       "        [0.20512094],\n",
       "        [0.07800519],\n",
       "        [0.05513156],\n",
       "        [0.13637248],\n",
       "        [0.15936035],\n",
       "        [0.13483278],\n",
       "        [0.46236232],\n",
       "        [0.9482844 ],\n",
       "        [0.7596464 ],\n",
       "        [0.11434424],\n",
       "        [0.26354122],\n",
       "        [0.5783602 ],\n",
       "        [0.96630627],\n",
       "        [0.47723424],\n",
       "        [0.36585322],\n",
       "        [0.9898717 ],\n",
       "        [0.31134334],\n",
       "        [0.1131723 ],\n",
       "        [0.41763815],\n",
       "        [0.03922066],\n",
       "        [0.8636203 ],\n",
       "        [0.68310004],\n",
       "        [0.98914623],\n",
       "        [0.14891247],\n",
       "        [0.7249623 ],\n",
       "        [0.27993226],\n",
       "        [0.44847178],\n",
       "        [0.9676992 ],\n",
       "        [0.16578183],\n",
       "        [0.6274636 ],\n",
       "        [0.9158599 ],\n",
       "        [0.08439071],\n",
       "        [0.9583995 ],\n",
       "        [0.33173645],\n",
       "        [0.7286506 ],\n",
       "        [0.05337284],\n",
       "        [0.28206095],\n",
       "        [0.8791222 ],\n",
       "        [0.04065252],\n",
       "        [0.09224422],\n",
       "        [0.57343644],\n",
       "        [0.9438163 ],\n",
       "        [0.7650499 ],\n",
       "        [0.31011218],\n",
       "        [0.96101755],\n",
       "        [0.18858847],\n",
       "        [0.08088309],\n",
       "        [0.9601377 ],\n",
       "        [0.88455254],\n",
       "        [0.79064804],\n",
       "        [0.81639946],\n",
       "        [0.03864261],\n",
       "        [0.58275586],\n",
       "        [0.14585231],\n",
       "        [0.8716435 ],\n",
       "        [0.46811026],\n",
       "        [0.9146716 ],\n",
       "        [0.07637841],\n",
       "        [0.06612226],\n",
       "        [0.11187933],\n",
       "        [0.7280387 ],\n",
       "        [0.3003936 ],\n",
       "        [0.32743847],\n",
       "        [0.48404053],\n",
       "        [0.38653773],\n",
       "        [0.9841629 ],\n",
       "        [0.7403773 ],\n",
       "        [0.6493104 ],\n",
       "        [0.7880035 ],\n",
       "        [0.371148  ],\n",
       "        [0.11457806],\n",
       "        [0.1692547 ],\n",
       "        [0.80037975],\n",
       "        [0.0556583 ],\n",
       "        [0.04977597],\n",
       "        [0.22669017],\n",
       "        [0.19009823],\n",
       "        [0.0402333 ],\n",
       "        [0.9030886 ],\n",
       "        [0.93727267],\n",
       "        [0.8719045 ],\n",
       "        [0.97105736],\n",
       "        [0.9823837 ],\n",
       "        [0.06064145],\n",
       "        [0.20630494],\n",
       "        [0.49504367],\n",
       "        [0.87681824],\n",
       "        [0.9827388 ],\n",
       "        [0.07227903],\n",
       "        [0.12415157],\n",
       "        [0.41873842],\n",
       "        [0.9887038 ],\n",
       "        [0.94274443],\n",
       "        [0.178912  ],\n",
       "        [0.21889089],\n",
       "        [0.92430913],\n",
       "        [0.03288671],\n",
       "        [0.21367934],\n",
       "        [0.91288126],\n",
       "        [0.59088176],\n",
       "        [0.3169296 ],\n",
       "        [0.80021846],\n",
       "        [0.588057  ],\n",
       "        [0.8448512 ],\n",
       "        [0.9791164 ],\n",
       "        [0.08451799],\n",
       "        [0.059706  ],\n",
       "        [0.19265486],\n",
       "        [0.06869315],\n",
       "        [0.35779798],\n",
       "        [0.891606  ],\n",
       "        [0.03737057],\n",
       "        [0.6756557 ],\n",
       "        [0.07312173],\n",
       "        [0.09828353],\n",
       "        [0.3278394 ],\n",
       "        [0.48530862],\n",
       "        [0.36421946],\n",
       "        [0.96514857],\n",
       "        [0.24083783],\n",
       "        [0.0876682 ],\n",
       "        [0.13158298],\n",
       "        [0.1280199 ],\n",
       "        [0.53349054],\n",
       "        [0.8686002 ],\n",
       "        [0.11259845],\n",
       "        [0.88537073],\n",
       "        [0.9643202 ],\n",
       "        [0.05185536],\n",
       "        [0.4272226 ],\n",
       "        [0.8369856 ],\n",
       "        [0.22024336],\n",
       "        [0.7114445 ],\n",
       "        [0.08609839],\n",
       "        [0.9542279 ],\n",
       "        [0.241159  ],\n",
       "        [0.16402632],\n",
       "        [0.13057478],\n",
       "        [0.06544594],\n",
       "        [0.18640928],\n",
       "        [0.6985787 ],\n",
       "        [0.95606524],\n",
       "        [0.14758569],\n",
       "        [0.9730069 ],\n",
       "        [0.665164  ],\n",
       "        [0.69124794],\n",
       "        [0.98378754],\n",
       "        [0.493714  ],\n",
       "        [0.43364683],\n",
       "        [0.8629528 ],\n",
       "        [0.9544597 ],\n",
       "        [0.06043019],\n",
       "        [0.21115987],\n",
       "        [0.07687654],\n",
       "        [0.23252957],\n",
       "        [0.37152717],\n",
       "        [0.92654955],\n",
       "        [0.8129548 ],\n",
       "        [0.96322423],\n",
       "        [0.08150034],\n",
       "        [0.03934404],\n",
       "        [0.85239106],\n",
       "        [0.08842276],\n",
       "        [0.06023467],\n",
       "        [0.05645632],\n",
       "        [0.12429323],\n",
       "        [0.54698056],\n",
       "        [0.2452624 ],\n",
       "        [0.5165559 ],\n",
       "        [0.22568472],\n",
       "        [0.19582093],\n",
       "        [0.13073693],\n",
       "        [0.24622703],\n",
       "        [0.4889973 ],\n",
       "        [0.9562085 ],\n",
       "        [0.7786939 ],\n",
       "        [0.7617046 ],\n",
       "        [0.9236423 ],\n",
       "        [0.94935465],\n",
       "        [0.0477845 ],\n",
       "        [0.85571337],\n",
       "        [0.06533846],\n",
       "        [0.9763916 ],\n",
       "        [0.0534203 ],\n",
       "        [0.19385825],\n",
       "        [0.26809776],\n",
       "        [0.04623808],\n",
       "        [0.18607916],\n",
       "        [0.29248124],\n",
       "        [0.95403624],\n",
       "        [0.8751912 ],\n",
       "        [0.85662806],\n",
       "        [0.08058843],\n",
       "        [0.78257495],\n",
       "        [0.7976319 ],\n",
       "        [0.08183743],\n",
       "        [0.2445356 ],\n",
       "        [0.8576912 ],\n",
       "        [0.6745947 ],\n",
       "        [0.9864893 ],\n",
       "        [0.2963297 ],\n",
       "        [0.16020516],\n",
       "        [0.6384366 ],\n",
       "        [0.10313226],\n",
       "        [0.7280573 ],\n",
       "        [0.92191666],\n",
       "        [0.22539888],\n",
       "        [0.05476729],\n",
       "        [0.06808125],\n",
       "        [0.958757  ],\n",
       "        [0.17763145],\n",
       "        [0.06141478],\n",
       "        [0.9720837 ],\n",
       "        [0.1730144 ],\n",
       "        [0.09626048],\n",
       "        [0.754941  ],\n",
       "        [0.02437976],\n",
       "        [0.26192176],\n",
       "        [0.3128911 ],\n",
       "        [0.64001346],\n",
       "        [0.01220974],\n",
       "        [0.30051288],\n",
       "        [0.94812477],\n",
       "        [0.13385554],\n",
       "        [0.94145346],\n",
       "        [0.9844864 ],\n",
       "        [0.05887939],\n",
       "        [0.06624147],\n",
       "        [0.04882561],\n",
       "        [0.9476275 ],\n",
       "        [0.536982  ],\n",
       "        [0.9804688 ],\n",
       "        [0.10805257],\n",
       "        [0.34156376],\n",
       "        [0.08440449],\n",
       "        [0.05386497],\n",
       "        [0.82405007],\n",
       "        [0.12458564],\n",
       "        [0.9346709 ],\n",
       "        [0.07547545],\n",
       "        [0.24903238],\n",
       "        [0.9834449 ],\n",
       "        [0.31957704],\n",
       "        [0.06672227],\n",
       "        [0.44398847],\n",
       "        [0.04326528],\n",
       "        [0.6433464 ],\n",
       "        [0.9846598 ],\n",
       "        [0.0754483 ],\n",
       "        [0.9873175 ],\n",
       "        [0.2655863 ],\n",
       "        [0.8564089 ],\n",
       "        [0.14376074],\n",
       "        [0.47249162],\n",
       "        [0.3597164 ],\n",
       "        [0.87415123],\n",
       "        [0.0275689 ],\n",
       "        [0.54693645],\n",
       "        [0.9292512 ],\n",
       "        [0.7077758 ],\n",
       "        [0.74506277],\n",
       "        [0.9300101 ],\n",
       "        [0.05365169],\n",
       "        [0.7714139 ],\n",
       "        [0.12890847],\n",
       "        [0.75050026],\n",
       "        [0.20110218],\n",
       "        [0.9277655 ],\n",
       "        [0.40048304],\n",
       "        [0.62343466],\n",
       "        [0.21910219],\n",
       "        [0.06609003],\n",
       "        [0.6345467 ],\n",
       "        [0.13669078],\n",
       "        [0.17366731],\n",
       "        [0.60560787],\n",
       "        [0.96587723],\n",
       "        [0.99035   ],\n",
       "        [0.06785403],\n",
       "        [0.07456015],\n",
       "        [0.06540119],\n",
       "        [0.12811536],\n",
       "        [0.2909508 ],\n",
       "        [0.05917278],\n",
       "        [0.97358626],\n",
       "        [0.11784058],\n",
       "        [0.49976718],\n",
       "        [0.09287586],\n",
       "        [0.11145759],\n",
       "        [0.8726329 ],\n",
       "        [0.11950584],\n",
       "        [0.09169394],\n",
       "        [0.27182457],\n",
       "        [0.09590751],\n",
       "        [0.47467464],\n",
       "        [0.9811509 ],\n",
       "        [0.56777775],\n",
       "        [0.04751576],\n",
       "        [0.16538702],\n",
       "        [0.1235268 ],\n",
       "        [0.03417966],\n",
       "        [0.9257025 ],\n",
       "        [0.2327043 ],\n",
       "        [0.7171546 ],\n",
       "        [0.49256948],\n",
       "        [0.1567785 ],\n",
       "        [0.436188  ],\n",
       "        [0.3216532 ],\n",
       "        [0.06081694],\n",
       "        [0.16260141],\n",
       "        [0.85768676],\n",
       "        [0.14626332],\n",
       "        [0.93067527],\n",
       "        [0.05836929],\n",
       "        [0.1994891 ],\n",
       "        [0.23252957],\n",
       "        [0.0427189 ],\n",
       "        [0.84952116],\n",
       "        [0.9621349 ],\n",
       "        [0.25641316],\n",
       "        [0.0690105 ],\n",
       "        [0.97502524],\n",
       "        [0.7763031 ],\n",
       "        [0.88321704],\n",
       "        [0.6363318 ],\n",
       "        [0.81928974],\n",
       "        [0.10010463],\n",
       "        [0.21235727],\n",
       "        [0.29020146],\n",
       "        [0.82592905],\n",
       "        [0.0896978 ],\n",
       "        [0.49556234],\n",
       "        [0.24815671],\n",
       "        [0.36880532],\n",
       "        [0.07639365],\n",
       "        [0.07527658],\n",
       "        [0.14526929],\n",
       "        [0.21738581],\n",
       "        [0.27424732],\n",
       "        [0.60386354],\n",
       "        [0.02103621],\n",
       "        [0.09496497],\n",
       "        [0.1951403 ],\n",
       "        [0.29504657],\n",
       "        [0.02394914],\n",
       "        [0.7270749 ],\n",
       "        [0.02972818],\n",
       "        [0.65445817],\n",
       "        [0.14746888],\n",
       "        [0.2569873 ],\n",
       "        [0.89481074],\n",
       "        [0.06885167],\n",
       "        [0.4190072 ],\n",
       "        [0.0825743 ],\n",
       "        [0.07841437],\n",
       "        [0.9626984 ],\n",
       "        [0.2761493 ],\n",
       "        [0.22125812],\n",
       "        [0.88492674],\n",
       "        [0.9470027 ],\n",
       "        [0.91451585],\n",
       "        [0.9887783 ],\n",
       "        [0.9649152 ],\n",
       "        [0.16485357],\n",
       "        [0.16221997],\n",
       "        [0.05090616],\n",
       "        [0.7339978 ],\n",
       "        [0.90470684],\n",
       "        [0.6229833 ],\n",
       "        [0.3905707 ],\n",
       "        [0.05507803],\n",
       "        [0.41283816],\n",
       "        [0.46314427],\n",
       "        [0.11510313],\n",
       "        [0.19041018],\n",
       "        [0.1263169 ],\n",
       "        [0.07928997],\n",
       "        [0.13745415],\n",
       "        [0.41280416],\n",
       "        [0.93489844],\n",
       "        [0.09213261],\n",
       "        [0.9403373 ],\n",
       "        [0.7374128 ],\n",
       "        [0.06281734],\n",
       "        [0.13108921],\n",
       "        [0.10476968],\n",
       "        [0.90258145],\n",
       "        [0.71395385],\n",
       "        [0.10233721]], dtype=float32),)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with USE TF Hub model\n",
    "model_6_pred_probs = model_6.predict(val_sentences),\n",
    "model_6_pred_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pred probs to labels\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 6 USE:\n",
      "{'accuracy': 81.23359580052494, 'precision': 0.8123900124632475, 'recall': 0.8123359580052494, 'f1-score': 0.8117376729557804}\n",
      "Baseline:\n",
      "{'accuracy': 79.26509186351706, 'precision': 0.8111390004213173, 'recall': 0.7926509186351706, 'f1-score': 0.7862189758049549}\n"
     ]
    }
   ],
   "source": [
    "# Calculate model 6 performance metrics\n",
    "model_6_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_6_preds\n",
    ")\n",
    "print(f\"Model 6 USE:\\n{model_6_results}\\nBaseline:\\n{baseline_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 7613, 7613)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(train_df), len(train_df_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7: TF Hub Pretrained USE but with 10% training data\n",
    "\n",
    "Transfer learning really helps when you don't have a large dataset.\n",
    "\n",
    "To see how our model performs on a smaller dataset, let's replicate `model_6` except we'll train it on 10% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è **Note:** Making data splits like below leads to data leakage (model_7 trained on 10% data, outperforms model_6 trained 100% data). DO NOT make data splits which leak data from validation/test set into training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS BAD!\n",
    "# Create subsets of 10% of the training data\n",
    "# train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n",
    "# train_10_percent.head(), len(train_10_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üîë **Note:** Be *very* careful when creating training/val/test splits that you don't leak data accross the dataset. Otherwise your model evaluation metrics will be wrong. If something looks to good to be true (a model trained on 10% of data outperforming the same model trained 100% of data) trust your gut and go back through to find where the error may lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685, 685)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a better dataset split (no data leakage)\n",
    "train_10_percent_split = int(0.1 * len(train_sentences)) # already in random order\n",
    "train_sentences_10_percent = train_sentences[:train_10_percent_split]\n",
    "train_labels_10_percent = train_labels[:train_10_percent_split]\n",
    "len(train_sentences_10_percent), len(train_labels_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n",
    "# train_labels_10_percent = train_10_percent[\"target\"].to_list()\n",
    "# len(train_sentences_10_percent), len(train_labels_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    406\n",
       "1    279\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.array(train_labels_10_percent)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of target in our subset of data\n",
    "train_df_shuffled[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recreate a model the same as a previous model we've created, we can use the `tf.keras.models.clone_model()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a model the same as model 6\n",
    "model_7 = tf.keras.models.clone_model(model_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model_7.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_USE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "USE (KerasLayer)             (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_7_USE_10_percent_correct_split/20220712-130741\n",
      "Epoch 1/5\n",
      "22/22 [==============================] - 6s 158ms/step - loss: 0.6651 - accuracy: 0.7358 - val_loss: 0.6419 - val_accuracy: 0.7402\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 1s 33ms/step - loss: 0.5905 - accuracy: 0.8044 - val_loss: 0.5833 - val_accuracy: 0.7664\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.5160 - accuracy: 0.8248 - val_loss: 0.5305 - val_accuracy: 0.7612\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.4586 - accuracy: 0.8219 - val_loss: 0.5006 - val_accuracy: 0.7756\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.4180 - accuracy: 0.8438 - val_loss: 0.4919 - val_accuracy: 0.7677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16f9a888880>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7.fit(\n",
    "    train_sentences_10_percent,\n",
    "    train_labels_10_percent,\n",
    "    epochs=5,\n",
    "    validation_data=(val_sentences, val_labels),\n",
    "    callbacks=[create_tensorboard_callback(\n",
    "        SAVE_DIR,\n",
    "        \"model_7_USE_10_percent_correct_split\"\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19485909],\n",
       "       [0.5355702 ],\n",
       "       [0.9061904 ],\n",
       "       [0.38565284],\n",
       "       [0.50257057],\n",
       "       [0.68120295],\n",
       "       [0.8921803 ],\n",
       "       [0.81791115],\n",
       "       [0.83766544],\n",
       "       [0.1498448 ]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7_pred_probs = model_7.predict(val_sentences)\n",
    "model_7_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
    "model_7_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.77165354330708,\n",
       " 'precision': 0.7691343474319641,\n",
       " 'recall': 0.7677165354330708,\n",
       " 'f1-score': 0.7656749923220023}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7_results = calculate_results(\n",
    "    y_true=val_labels,\n",
    "    y_pred=model_7_preds\n",
    ")\n",
    "model_7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 81.23359580052494,\n",
       " 'precision': 0.8123900124632475,\n",
       " 'recall': 0.8123359580052494,\n",
       " 'f1-score': 0.8117376729557804}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1-score': 0.7862189758049549}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance of each of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine model results into a DataFrame\n",
    "all_model_results = pd.DataFrame({\n",
    "    \"baseline\": baseline_results,\n",
    "    \"1_simple_dense\": model_1_results,\n",
    "    \"2_LSTM\": model_2_results,\n",
    "    \"3_GRU\": model_3_results,\n",
    "    \"4_bidirectional\": model_4_results,\n",
    "    \"5_conv1D\": model_5_results,\n",
    "    \"6_tf_hub_USE\": model_6_results,\n",
    "    \"7_tf_hub_USE_10_perc\": model_7_results    \n",
    "})\n",
    "all_model_results = all_model_results.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.811139</td>\n",
       "      <td>0.792651</td>\n",
       "      <td>0.786219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_simple_dense</th>\n",
       "      <td>0.780840</td>\n",
       "      <td>0.782334</td>\n",
       "      <td>0.780840</td>\n",
       "      <td>0.779034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_LSTM</th>\n",
       "      <td>0.769029</td>\n",
       "      <td>0.770603</td>\n",
       "      <td>0.769029</td>\n",
       "      <td>0.766934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_GRU</th>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.781410</td>\n",
       "      <td>0.778215</td>\n",
       "      <td>0.775608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_bidirectional</th>\n",
       "      <td>0.754593</td>\n",
       "      <td>0.754230</td>\n",
       "      <td>0.754593</td>\n",
       "      <td>0.753716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_conv1D</th>\n",
       "      <td>0.758530</td>\n",
       "      <td>0.759033</td>\n",
       "      <td>0.758530</td>\n",
       "      <td>0.756855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_tf_hub_USE</th>\n",
       "      <td>0.812336</td>\n",
       "      <td>0.812390</td>\n",
       "      <td>0.812336</td>\n",
       "      <td>0.811738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_tf_hub_USE_10_perc</th>\n",
       "      <td>0.767717</td>\n",
       "      <td>0.769134</td>\n",
       "      <td>0.767717</td>\n",
       "      <td>0.765675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      accuracy  precision    recall  f1-score\n",
       "baseline              0.792651   0.811139  0.792651  0.786219\n",
       "1_simple_dense        0.780840   0.782334  0.780840  0.779034\n",
       "2_LSTM                0.769029   0.770603  0.769029  0.766934\n",
       "3_GRU                 0.778215   0.781410  0.778215  0.775608\n",
       "4_bidirectional       0.754593   0.754230  0.754593  0.753716\n",
       "5_conv1D              0.758530   0.759033  0.758530  0.756855\n",
       "6_tf_hub_USE          0.812336   0.812390  0.812336  0.811738\n",
       "7_tf_hub_USE_10_perc  0.767717   0.769134  0.767717  0.765675"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the accuracyas the same scale as other metrics\n",
    "all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16fbc41c070>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"681.421875pt\" height=\"514.343437pt\" viewBox=\"0 0 681.421875 514.343437\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-07-12T13:07:50.856397</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 514.343437 \nL 681.421875 514.343437 \nL 681.421875 0 \nL -0 0 \nz\n\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 387.72 \nL 588.103125 387.72 \nL 588.103125 7.2 \nL 30.103125 7.2 \nz\n\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 47.540625 387.72 \nL 56.259375 387.72 \nL 56.259375 34.125435 \nL 47.540625 34.125435 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 117.290625 387.72 \nL 126.009375 387.72 \nL 126.009375 39.394228 \nL 117.290625 39.394228 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 187.040625 387.72 \nL 195.759375 387.72 \nL 195.759375 44.663022 \nL 187.040625 44.663022 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 256.790625 387.72 \nL 265.509375 387.72 \nL 265.509375 40.565071 \nL 256.790625 40.565071 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 326.540625 387.72 \nL 335.259375 387.72 \nL 335.259375 51.102658 \nL 326.540625 51.102658 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 396.290625 387.72 \nL 405.009375 387.72 \nL 405.009375 49.346393 \nL 396.290625 49.346393 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 466.040625 387.72 \nL 474.759375 387.72 \nL 474.759375 25.344113 \nL 466.040625 25.344113 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 535.790625 387.72 \nL 544.509375 387.72 \nL 544.509375 45.248443 \nL 535.790625 45.248443 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 56.259375 387.72 \nL 64.978125 387.72 \nL 64.978125 25.878065 \nL 56.259375 25.878065 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #feffb3\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 126.009375 387.72 \nL 134.728125 387.72 \nL 134.728125 38.727629 \nL 126.009375 38.727629 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #feffb3\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 195.759375 387.72 \nL 204.478125 387.72 \nL 204.478125 43.960903 \nL 195.759375 43.960903 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #feffb3\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 265.509375 387.72 \nL 274.228125 387.72 \nL 274.228125 39.139763 \nL 265.509375 39.139763 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #feffb3\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 335.259375 387.72 \nL 343.978125 387.72 \nL 343.978125 51.26468 \nL 335.259375 51.26468 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #feffb3\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 405.009375 387.72 \nL 413.728125 387.72 \nL 413.728125 49.121901 \nL 405.009375 49.121901 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #feffb3\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path d=\"M 474.759375 387.72 \nL 483.478125 387.72 \nL 483.478125 25.32 \nL 474.759375 25.32 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #feffb3\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 544.509375 387.72 \nL 553.228125 387.72 \nL 553.228125 44.61597 \nL 544.509375 44.61597 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #feffb3\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 64.978125 387.72 \nL 73.696875 387.72 \nL 73.696875 34.125435 \nL 64.978125 34.125435 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #bfbbd9\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 134.728125 387.72 \nL 143.446875 387.72 \nL 143.446875 39.394228 \nL 134.728125 39.394228 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #bfbbd9\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 204.478125 387.72 \nL 213.196875 387.72 \nL 213.196875 44.663022 \nL 204.478125 44.663022 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #bfbbd9\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path d=\"M 274.228125 387.72 \nL 282.946875 387.72 \nL 282.946875 40.565071 \nL 274.228125 40.565071 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #bfbbd9\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 343.978125 387.72 \nL 352.696875 387.72 \nL 352.696875 51.102658 \nL 343.978125 51.102658 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #bfbbd9\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 413.728125 387.72 \nL 422.446875 387.72 \nL 422.446875 49.346393 \nL 413.728125 49.346393 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #bfbbd9\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 483.478125 387.72 \nL 492.196875 387.72 \nL 492.196875 25.344113 \nL 483.478125 25.344113 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #bfbbd9\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 553.228125 387.72 \nL 561.946875 387.72 \nL 561.946875 45.248443 \nL 553.228125 45.248443 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #bfbbd9\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path d=\"M 73.696875 387.72 \nL 82.415625 387.72 \nL 82.415625 36.994668 \nL 73.696875 36.994668 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #fa8174\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 143.446875 387.72 \nL 152.165625 387.72 \nL 152.165625 40.199883 \nL 143.446875 40.199883 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #fa8174\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 213.196875 387.72 \nL 221.915625 387.72 \nL 221.915625 45.597421 \nL 213.196875 45.597421 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #fa8174\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 282.946875 387.72 \nL 291.665625 387.72 \nL 291.665625 41.728352 \nL 282.946875 41.728352 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #fa8174\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 352.696875 387.72 \nL 361.415625 387.72 \nL 361.415625 51.493934 \nL 352.696875 51.493934 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #fa8174\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path d=\"M 422.446875 387.72 \nL 431.165625 387.72 \nL 431.165625 50.09346 \nL 422.446875 50.09346 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #fa8174\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 492.196875 387.72 \nL 500.915625 387.72 \nL 500.915625 25.611003 \nL 492.196875 25.611003 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #fa8174\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 561.946875 387.72 \nL 570.665625 387.72 \nL 570.665625 46.159157 \nL 561.946875 46.159157 \nz\n\" clip-path=\"url(#pffc90a5c31)\" style=\"fill: #fa8174\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m2caea31de1\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #ffffff; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m2caea31de1\" x=\"64.978125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- baseline -->\n      <g style=\"fill: #ffffff\" transform=\"translate(67.7375 436.605937)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-62\"/>\n       <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-73\" x=\"124.755859\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"176.855469\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"238.378906\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"266.162109\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"293.945312\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"357.324219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m2caea31de1\" x=\"134.728125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1_simple_dense -->\n      <g style=\"fill: #ffffff\" transform=\"translate(137.348438 474.291875)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-73\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"165.722656\"/>\n       <use xlink:href=\"#DejaVuSans-6d\" x=\"193.505859\"/>\n       <use xlink:href=\"#DejaVuSans-70\" x=\"290.917969\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"354.394531\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"382.177734\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"443.701172\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"493.701172\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"557.177734\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"618.701172\"/>\n       <use xlink:href=\"#DejaVuSans-73\" x=\"682.080078\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"734.179688\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m2caea31de1\" x=\"204.478125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2_LSTM -->\n      <g style=\"fill: #ffffff\" transform=\"translate(207.010156 432.73875)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-4c\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-53\" x=\"169.335938\"/>\n       <use xlink:href=\"#DejaVuSans-54\" x=\"232.8125\"/>\n       <use xlink:href=\"#DejaVuSans-4d\" x=\"293.896484\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m2caea31de1\" x=\"274.228125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3_GRU -->\n      <g style=\"fill: #ffffff\" transform=\"translate(276.760156 428.098125)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-47\" d=\"M 3809 666 \nL 3809 1919 \nL 2778 1919 \nL 2778 2438 \nL 4434 2438 \nL 4434 434 \nQ 4069 175 3628 42 \nQ 3188 -91 2688 -91 \nQ 1594 -91 976 548 \nQ 359 1188 359 2328 \nQ 359 3472 976 4111 \nQ 1594 4750 2688 4750 \nQ 3144 4750 3555 4637 \nQ 3966 4525 4313 4306 \nL 4313 3634 \nQ 3963 3931 3569 4081 \nQ 3175 4231 2741 4231 \nQ 1884 4231 1454 3753 \nQ 1025 3275 1025 2328 \nQ 1025 1384 1454 906 \nQ 1884 428 2741 428 \nQ 3075 428 3337 486 \nQ 3600 544 3809 666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-52\" d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-55\" d=\"M 556 4666 \nL 1191 4666 \nL 1191 1831 \nQ 1191 1081 1462 751 \nQ 1734 422 2344 422 \nQ 2950 422 3222 751 \nQ 3494 1081 3494 1831 \nL 3494 4666 \nL 4128 4666 \nL 4128 1753 \nQ 4128 841 3676 375 \nQ 3225 -91 2344 -91 \nQ 1459 -91 1007 375 \nQ 556 841 556 1753 \nL 556 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-47\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-52\" x=\"191.113281\"/>\n       <use xlink:href=\"#DejaVuSans-55\" x=\"260.595703\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m2caea31de1\" x=\"343.978125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4_bidirectional -->\n      <g style=\"fill: #ffffff\" transform=\"translate(346.598437 467.934062)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-62\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"177.099609\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"204.882812\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"268.359375\"/>\n       <use xlink:href=\"#DejaVuSans-72\" x=\"296.142578\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"335.005859\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"396.529297\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"451.509766\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"490.71875\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"518.501953\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"579.683594\"/>\n       <use xlink:href=\"#DejaVuSans-61\" x=\"643.0625\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"704.341797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m2caea31de1\" x=\"413.728125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 5_conv1D -->\n      <g style=\"fill: #ffffff\" transform=\"translate(416.209375 444.018437)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-44\" d=\"M 1259 4147 \nL 1259 519 \nL 2022 519 \nQ 2988 519 3436 956 \nQ 3884 1394 3884 2338 \nQ 3884 3275 3436 3711 \nQ 2988 4147 2022 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 1925 4666 \nQ 3281 4666 3915 4102 \nQ 4550 3538 4550 2338 \nQ 4550 1131 3912 565 \nQ 3275 0 1925 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"168.603516\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"229.785156\"/>\n       <use xlink:href=\"#DejaVuSans-76\" x=\"293.164062\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"352.34375\"/>\n       <use xlink:href=\"#DejaVuSans-44\" x=\"415.966797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m2caea31de1\" x=\"483.478125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 6_tf_hub_USE -->\n      <g style=\"fill: #ffffff\" transform=\"translate(486.098437 462.5325)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-66\" x=\"152.832031\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"188.037109\"/>\n       <use xlink:href=\"#DejaVuSans-68\" x=\"238.037109\"/>\n       <use xlink:href=\"#DejaVuSans-75\" x=\"301.416016\"/>\n       <use xlink:href=\"#DejaVuSans-62\" x=\"364.794922\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"428.271484\"/>\n       <use xlink:href=\"#DejaVuSans-55\" x=\"478.271484\"/>\n       <use xlink:href=\"#DejaVuSans-53\" x=\"551.464844\"/>\n       <use xlink:href=\"#DejaVuSans-45\" x=\"614.941406\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m2caea31de1\" x=\"553.228125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 7_tf_hub_USE_10_perc -->\n      <g style=\"fill: #ffffff\" transform=\"translate(555.848437 507.143437)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-66\" x=\"152.832031\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"188.037109\"/>\n       <use xlink:href=\"#DejaVuSans-68\" x=\"238.037109\"/>\n       <use xlink:href=\"#DejaVuSans-75\" x=\"301.416016\"/>\n       <use xlink:href=\"#DejaVuSans-62\" x=\"364.794922\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"428.271484\"/>\n       <use xlink:href=\"#DejaVuSans-55\" x=\"478.271484\"/>\n       <use xlink:href=\"#DejaVuSans-53\" x=\"551.464844\"/>\n       <use xlink:href=\"#DejaVuSans-45\" x=\"614.941406\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"678.125\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"728.125\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"791.748047\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"855.371094\"/>\n       <use xlink:href=\"#DejaVuSans-70\" x=\"905.371094\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"968.847656\"/>\n       <use xlink:href=\"#DejaVuSans-72\" x=\"1030.371094\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"1069.234375\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path id=\"m258f34d323\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #ffffff; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 391.519219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"343.110884\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.1 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 346.910103)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"298.501769\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.2 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 302.300987)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"253.892653\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.3 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 257.691872)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"209.283537\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.4 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 213.082756)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"164.674422\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.5 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 168.473641)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"120.065306\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.6 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 123.864525)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"75.456191\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.7 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 79.255409)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#m258f34d323\" x=\"30.103125\" y=\"30.847075\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.8 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 34.646294)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 30.103125 387.72 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 588.103125 387.72 \nL 588.103125 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path d=\"M 30.103125 387.72 \nL 588.103125 387.72 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 30.103125 7.2 \nL 588.103125 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_39\">\n     <path d=\"M 595.103125 73.9125 \nL 672.221875 73.9125 \nQ 674.221875 73.9125 674.221875 71.9125 \nL 674.221875 14.2 \nQ 674.221875 12.2 672.221875 12.2 \nL 595.103125 12.2 \nQ 593.103125 12.2 593.103125 14.2 \nL 593.103125 71.9125 \nQ 593.103125 73.9125 595.103125 73.9125 \nz\n\" style=\"opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"patch_40\">\n     <path d=\"M 597.103125 23.798437 \nL 617.103125 23.798437 \nL 617.103125 16.798437 \nL 597.103125 16.798437 \nz\n\" style=\"fill: #8dd3c7\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- accuracy -->\n     <g style=\"fill: #ffffff\" transform=\"translate(625.103125 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"61.279297\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"116.259766\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"171.240234\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"234.619141\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"275.732422\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"337.011719\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"391.992188\"/>\n     </g>\n    </g>\n    <g id=\"patch_41\">\n     <path d=\"M 597.103125 38.476562 \nL 617.103125 38.476562 \nL 617.103125 31.476562 \nL 597.103125 31.476562 \nz\n\" style=\"fill: #feffb3\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- precision -->\n     <g style=\"fill: #ffffff\" transform=\"translate(625.103125 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"63.476562\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"102.339844\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"163.863281\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"218.84375\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"246.626953\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"298.726562\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"326.509766\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"387.691406\"/>\n     </g>\n    </g>\n    <g id=\"patch_42\">\n     <path d=\"M 597.103125 53.154687 \nL 617.103125 53.154687 \nL 617.103125 46.154687 \nL 597.103125 46.154687 \nz\n\" style=\"fill: #bfbbd9\"/>\n    </g>\n    <g id=\"text_20\">\n     <!-- recall -->\n     <g style=\"fill: #ffffff\" transform=\"translate(625.103125 53.154687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"38.863281\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"100.386719\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"155.367188\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"216.646484\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"244.429688\"/>\n     </g>\n    </g>\n    <g id=\"patch_43\">\n     <path d=\"M 597.103125 67.832812 \nL 617.103125 67.832812 \nL 617.103125 60.832812 \nL 597.103125 60.832812 \nz\n\" style=\"fill: #fa8174\"/>\n    </g>\n    <g id=\"text_21\">\n     <!-- f1-score -->\n     <g style=\"fill: #ffffff\" transform=\"translate(625.103125 67.832812)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-2d\" d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-66\"/>\n      <use xlink:href=\"#DejaVuSans-31\" x=\"35.205078\"/>\n      <use xlink:href=\"#DejaVuSans-2d\" x=\"98.828125\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"134.912109\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"187.011719\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"241.992188\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"303.173828\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"342.037109\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pffc90a5c31\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"558\" height=\"380.52\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot and compare all of the model results\n",
    "all_model_results.plot(\n",
    "    kind=\"bar\", \n",
    "    figsize=(10,7)\n",
    ").legend(bbox_to_anchor=(1.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"595.303125pt\" height=\"514.343437pt\" viewBox=\"0 0 595.303125 514.343437\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-07-12T13:07:51.230962</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 514.343437 \nL 595.303125 514.343437 \nL 595.303125 0 \nL -0 0 \nz\n\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 387.72 \nL 588.103125 387.72 \nL 588.103125 7.2 \nL 30.103125 7.2 \nz\n\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 47.540625 387.72 \nL 82.415625 387.72 \nL 82.415625 25.32 \nL 47.540625 25.32 \nz\n\" clip-path=\"url(#p0bc195cc25)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 117.290625 387.72 \nL 152.165625 387.72 \nL 152.165625 36.712813 \nL 117.290625 36.712813 \nz\n\" clip-path=\"url(#p0bc195cc25)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 187.040625 387.72 \nL 221.915625 387.72 \nL 221.915625 39.920604 \nL 187.040625 39.920604 \nz\n\" clip-path=\"url(#p0bc195cc25)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 256.790625 387.72 \nL 291.665625 387.72 \nL 291.665625 41.450302 \nL 256.790625 41.450302 \nz\n\" clip-path=\"url(#p0bc195cc25)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 326.540625 387.72 \nL 361.415625 387.72 \nL 361.415625 45.322479 \nL 326.540625 45.322479 \nz\n\" clip-path=\"url(#p0bc195cc25)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 396.290625 387.72 \nL 431.165625 387.72 \nL 431.165625 45.884668 \nL 396.290625 45.884668 \nz\n\" clip-path=\"url(#p0bc195cc25)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 466.040625 387.72 \nL 500.915625 387.72 \nL 500.915625 49.822132 \nL 466.040625 49.822132 \nz\n\" clip-path=\"url(#p0bc195cc25)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 535.790625 387.72 \nL 570.665625 387.72 \nL 570.665625 51.223731 \nL 535.790625 51.223731 \nz\n\" clip-path=\"url(#p0bc195cc25)\" style=\"fill: #8dd3c7\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m669cb0cd00\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #ffffff; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m669cb0cd00\" x=\"64.978125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 6_tf_hub_USE -->\n      <g style=\"fill: #ffffff\" transform=\"translate(67.598438 462.5325)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-55\" d=\"M 556 4666 \nL 1191 4666 \nL 1191 1831 \nQ 1191 1081 1462 751 \nQ 1734 422 2344 422 \nQ 2950 422 3222 751 \nQ 3494 1081 3494 1831 \nL 3494 4666 \nL 4128 4666 \nL 4128 1753 \nQ 4128 841 3676 375 \nQ 3225 -91 2344 -91 \nQ 1459 -91 1007 375 \nQ 556 841 556 1753 \nL 556 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-45\" d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-66\" x=\"152.832031\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"188.037109\"/>\n       <use xlink:href=\"#DejaVuSans-68\" x=\"238.037109\"/>\n       <use xlink:href=\"#DejaVuSans-75\" x=\"301.416016\"/>\n       <use xlink:href=\"#DejaVuSans-62\" x=\"364.794922\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"428.271484\"/>\n       <use xlink:href=\"#DejaVuSans-55\" x=\"478.271484\"/>\n       <use xlink:href=\"#DejaVuSans-53\" x=\"551.464844\"/>\n       <use xlink:href=\"#DejaVuSans-45\" x=\"614.941406\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m669cb0cd00\" x=\"134.728125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- baseline -->\n      <g style=\"fill: #ffffff\" transform=\"translate(137.4875 436.605937)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-62\"/>\n       <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n       <use xlink:href=\"#DejaVuSans-73\" x=\"124.755859\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"176.855469\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"238.378906\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"266.162109\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"293.945312\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"357.324219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m669cb0cd00\" x=\"204.478125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 1_simple_dense -->\n      <g style=\"fill: #ffffff\" transform=\"translate(207.098438 474.291875)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-73\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"165.722656\"/>\n       <use xlink:href=\"#DejaVuSans-6d\" x=\"193.505859\"/>\n       <use xlink:href=\"#DejaVuSans-70\" x=\"290.917969\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"354.394531\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"382.177734\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"443.701172\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"493.701172\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"557.177734\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"618.701172\"/>\n       <use xlink:href=\"#DejaVuSans-73\" x=\"682.080078\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"734.179688\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m669cb0cd00\" x=\"274.228125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3_GRU -->\n      <g style=\"fill: #ffffff\" transform=\"translate(276.760156 428.098125)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-47\" d=\"M 3809 666 \nL 3809 1919 \nL 2778 1919 \nL 2778 2438 \nL 4434 2438 \nL 4434 434 \nQ 4069 175 3628 42 \nQ 3188 -91 2688 -91 \nQ 1594 -91 976 548 \nQ 359 1188 359 2328 \nQ 359 3472 976 4111 \nQ 1594 4750 2688 4750 \nQ 3144 4750 3555 4637 \nQ 3966 4525 4313 4306 \nL 4313 3634 \nQ 3963 3931 3569 4081 \nQ 3175 4231 2741 4231 \nQ 1884 4231 1454 3753 \nQ 1025 3275 1025 2328 \nQ 1025 1384 1454 906 \nQ 1884 428 2741 428 \nQ 3075 428 3337 486 \nQ 3600 544 3809 666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-52\" d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-47\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-52\" x=\"191.113281\"/>\n       <use xlink:href=\"#DejaVuSans-55\" x=\"260.595703\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m669cb0cd00\" x=\"343.978125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2_LSTM -->\n      <g style=\"fill: #ffffff\" transform=\"translate(346.510156 432.73875)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-4c\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-53\" x=\"169.335938\"/>\n       <use xlink:href=\"#DejaVuSans-54\" x=\"232.8125\"/>\n       <use xlink:href=\"#DejaVuSans-4d\" x=\"293.896484\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m669cb0cd00\" x=\"413.728125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 7_tf_hub_USE_10_perc -->\n      <g style=\"fill: #ffffff\" transform=\"translate(416.348437 507.143437)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-66\" x=\"152.832031\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"188.037109\"/>\n       <use xlink:href=\"#DejaVuSans-68\" x=\"238.037109\"/>\n       <use xlink:href=\"#DejaVuSans-75\" x=\"301.416016\"/>\n       <use xlink:href=\"#DejaVuSans-62\" x=\"364.794922\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"428.271484\"/>\n       <use xlink:href=\"#DejaVuSans-55\" x=\"478.271484\"/>\n       <use xlink:href=\"#DejaVuSans-53\" x=\"551.464844\"/>\n       <use xlink:href=\"#DejaVuSans-45\" x=\"614.941406\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"678.125\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"728.125\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"791.748047\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"855.371094\"/>\n       <use xlink:href=\"#DejaVuSans-70\" x=\"905.371094\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"968.847656\"/>\n       <use xlink:href=\"#DejaVuSans-72\" x=\"1030.371094\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"1069.234375\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m669cb0cd00\" x=\"483.478125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5_conv1D -->\n      <g style=\"fill: #ffffff\" transform=\"translate(485.959375 444.018437)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-44\" d=\"M 1259 4147 \nL 1259 519 \nL 2022 519 \nQ 2988 519 3436 956 \nQ 3884 1394 3884 2338 \nQ 3884 3275 3436 3711 \nQ 2988 4147 2022 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 1925 4666 \nQ 3281 4666 3915 4102 \nQ 4550 3538 4550 2338 \nQ 4550 1131 3912 565 \nQ 3275 0 1925 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"168.603516\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"229.785156\"/>\n       <use xlink:href=\"#DejaVuSans-76\" x=\"293.164062\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"352.34375\"/>\n       <use xlink:href=\"#DejaVuSans-44\" x=\"415.966797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m669cb0cd00\" x=\"553.228125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 4_bidirectional -->\n      <g style=\"fill: #ffffff\" transform=\"translate(555.848437 467.934062)rotate(-90)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-5f\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-62\" x=\"113.623047\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"177.099609\"/>\n       <use xlink:href=\"#DejaVuSans-64\" x=\"204.882812\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"268.359375\"/>\n       <use xlink:href=\"#DejaVuSans-72\" x=\"296.142578\"/>\n       <use xlink:href=\"#DejaVuSans-65\" x=\"335.005859\"/>\n       <use xlink:href=\"#DejaVuSans-63\" x=\"396.529297\"/>\n       <use xlink:href=\"#DejaVuSans-74\" x=\"451.509766\"/>\n       <use xlink:href=\"#DejaVuSans-69\" x=\"490.71875\"/>\n       <use xlink:href=\"#DejaVuSans-6f\" x=\"518.501953\"/>\n       <use xlink:href=\"#DejaVuSans-6e\" x=\"579.683594\"/>\n       <use xlink:href=\"#DejaVuSans-61\" x=\"643.0625\"/>\n       <use xlink:href=\"#DejaVuSans-6c\" x=\"704.341797\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path id=\"m9f3b16ed57\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #ffffff; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"387.72\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 391.519219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"343.075035\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.1 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 346.874254)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"298.43007\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.2 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 302.229289)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"253.785105\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.3 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 257.584324)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"209.14014\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.4 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 212.939359)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"164.495175\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.5 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 168.294394)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"119.85021\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.6 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 123.649429)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"75.205245\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.7 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 79.004464)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#m9f3b16ed57\" x=\"30.103125\" y=\"30.56028\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.8 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 34.359499)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 30.103125 387.72 \nL 30.103125 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 588.103125 387.72 \nL 588.103125 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 30.103125 387.72 \nL 588.103125 387.72 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 30.103125 7.2 \nL 588.103125 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0bc195cc25\">\n   <rect x=\"30.103125\" y=\"7.2\" width=\"558\" height=\"380.52\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort model results by f1-score\n",
    "all_model_results.sort_values(\"f1-score\", ascending=False)[\"f1-score\"].plot(kind=\"bar\", figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading our model training logs to TensorBoard.dev\n",
    "\n",
    "We can further inspect our model's performance using TensorBoard.dev: https://tensordboard.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#%reload_ext tensorboard\n",
    "#!tensorboard --logdir ./model_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading a trained model\n",
    "\n",
    "There are two main formats to save a model to in TensorFlow:\n",
    "1. HDF5 format\n",
    "2. `SavedModel` format (this is the default when using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF Hub Sentence Encoder model to HDF5 format\n",
    "model_6.save('model_6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üîë **Note:** If loading doesn't work because of OOM, restart kernel and start from here. You also need to import the data again (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with custom Hab Layer (required HDF5 format)\n",
    "#loaded_model_6 = tf.keras.models.load_model(\n",
    "#    \"model_6.h5\",\n",
    "#    custom_objects={\"KerasLayer\": hub.KerasLayer}\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) USE_input with unsupported characters which will be renamed to use_input in the SavedModel.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[26667,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ReadVariableOp]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31492/3442745809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_6_SavedModel_format\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \"\"\"\n\u001b[0;32m   2144\u001b[0m     \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2145\u001b[1;33m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[0;32m   2146\u001b[0m                     signatures, options, save_traces)\n\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    147\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSharedObjectSavingScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m       saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[0m\u001b[0;32m    150\u001b[0m                             signatures, options, save_traces)\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\keras\\saving\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated_internal_learning_phase_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_traces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m       saved_nodes, node_paths = save_lib.save_and_return_nodes(\n\u001b[0m\u001b[0;32m     91\u001b[0m           model, filepath, signatures, options)\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36msave_and_return_nodes\u001b[1;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[0;32m   1236\u001b[0m     ckpt_options = checkpoint_options.CheckpointOptions(\n\u001b[0;32m   1237\u001b[0m         experimental_io_device=options.experimental_io_device)\n\u001b[1;32m-> 1238\u001b[1;33m     object_saver.save(\n\u001b[0m\u001b[0;32m   1239\u001b[0m         utils_impl.get_variables_path(export_dir), options=ckpt_options)\n\u001b[0;32m   1240\u001b[0m     builder_impl.copy_assets_to_destination_dir(asset_info.asset_filename_map,\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session, options)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[0m\u001b[0;32m   1263\u001b[0m         file_prefix_tensor, object_graph_tensor, options)\n\u001b[0;32m   1264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36m_save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor, options)\u001b[0m\n\u001b[0;32m   1206\u001b[0m         or context.executing_eagerly() or ops.inside_function()):\n\u001b[0;32m   1207\u001b[0m       \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDeviceSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_saveable_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m       \u001b[0msave_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[0mtf_function_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msave_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    272\u001b[0m           \u001b[1;31m# initial read operations should be placed on the SaveableObject's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m           \u001b[1;31m# device.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m           \u001b[0msharded_saves\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshard_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msharded_saves\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msaveable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_saveable_objects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msaveable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;31m# A tensor value of `None` indicates that this SaveableObject gets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# recorded in the object graph, but that no value is saved in the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object.py\u001b[0m in \u001b[0;36mtensor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m               \u001b[1;31m# uninitialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m               \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[1;31m# To allow variables placed on non-CPU devices to be checkpointed,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;31m# we copy them to CPU on the same machine first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \"\"\"\n\u001b[0;32m    703\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    681\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[1;34m()\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[0;32m    674\u001b[0m           self.handle, self._dtype)\n\u001b[0;32m    675\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    471\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gutsc\\anaconda3\\envs\\udemy_tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6940\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6941\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6942\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[26667,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ReadVariableOp]"
     ]
    }
   ],
   "source": [
    "model_6.save(\"model_6_SavedModel_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('udemy_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63418e73e250c304d26eaf03185e2f59ff2277f62dd1cc8bba54019c2c46cc60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
